{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Condon Research Group Docs This is a repository for holding all things documentation that might be helpful. It has been created using mkdocs . Contributing To contribute first you need to clone this repo git clone git@github.com:condon-lab/Condon_Lab_Docs.git Then you'll need to install the mkdocs package by running pip install mkdocs Checkout a new branch where you can add/edit docs git checkout -b <branch-name> Then edit docs to your heart's content! If you prefer to write documentation using google docs, you can convert a google doc to a markdown file easily using this chrome extension . Additionally, here are some helpful markdown tips and tricks . If you want to add an image to your documentation simply add the image you want to the ./images directory and then reference the image using the relative path. e.g. [alt text for your image](../../images/my_image.png) If you remove an image from an existing doc, please remove the image file also. You can then run mkdocs serve to see a local version of your new and updated docs in you browse at http://127.0.0.1:8000/ When everything looks good open a pull request to main and either ping in slack or assign Laura and/or Will to review. After your PR has been appoved, merge it in and go check out your new docs over at https://condon-lab.github.io/Condon_Lab_Docs/","title":"Condon Research Group Docs"},{"location":"#condon-research-group-docs","text":"This is a repository for holding all things documentation that might be helpful. It has been created using mkdocs .","title":"Condon Research Group Docs"},{"location":"#contributing","text":"To contribute first you need to clone this repo git clone git@github.com:condon-lab/Condon_Lab_Docs.git Then you'll need to install the mkdocs package by running pip install mkdocs Checkout a new branch where you can add/edit docs git checkout -b <branch-name> Then edit docs to your heart's content! If you prefer to write documentation using google docs, you can convert a google doc to a markdown file easily using this chrome extension . Additionally, here are some helpful markdown tips and tricks . If you want to add an image to your documentation simply add the image you want to the ./images directory and then reference the image using the relative path. e.g. [alt text for your image](../../images/my_image.png) If you remove an image from an existing doc, please remove the image file also. You can then run mkdocs serve to see a local version of your new and updated docs in you browse at http://127.0.0.1:8000/ When everything looks good open a pull request to main and either ping in slack or assign Laura and/or Will to review. After your PR has been appoved, merge it in and go check out your new docs over at https://condon-lab.github.io/Condon_Lab_Docs/","title":"Contributing"},{"location":"admin/pcard/","text":"Information Regarding Using the Pcard: Genaral procedure Email Erma Santander ermasan@arizona.edu before the p-card is used, so she can log every purchase and keep an eye out for fraud. She will require the signed authorization form before the p-card can be used. The person using the p-card would then send Cindy Barnett barnettc@arizona.edu their receipt and the HAS P-card form pcard_authorization_form) right away. Cindy has 10 business days after the p-card purchase hits our account to approve and enter the corresponding account information. Pcard policy indicates Erma is the pcard holder. She is the only authorized person to release the pcard out for use. For audit purposes, the Pcards must be checked out and logged at every use; the consequence being the removal of our department pcard. Failure to following these instructions will result in an individual\u2019s pcard privileges discontinued. Details Obtaining the Pcard for Use: - Over the Phone Contact Erma Santander at 520-621-7120, Monday thru Thursday from 10am \u2013 4pm, to obtain pcard information and a log number for a one-time use. If at one time you were given the pcard number to purchase an item, you are not authorized to keep this number written down or to use it multiple times. - Physical Card Two department pcards are available for checkout Monday thru Thursday from 10am -4pm in Harshbarger Room 202, with a maximum checkout time of no more than two hours. No overnight or weekend checkouts are permitted. When a pcard is checked out you will receive a log number for its one-time use.","title":"Pcard"},{"location":"admin/pcard/#information-regarding-using-the-pcard","text":"","title":"Information Regarding Using the Pcard:"},{"location":"admin/pcard/#genaral-procedure","text":"Email Erma Santander ermasan@arizona.edu before the p-card is used, so she can log every purchase and keep an eye out for fraud. She will require the signed authorization form before the p-card can be used. The person using the p-card would then send Cindy Barnett barnettc@arizona.edu their receipt and the HAS P-card form pcard_authorization_form) right away. Cindy has 10 business days after the p-card purchase hits our account to approve and enter the corresponding account information. Pcard policy indicates Erma is the pcard holder. She is the only authorized person to release the pcard out for use. For audit purposes, the Pcards must be checked out and logged at every use; the consequence being the removal of our department pcard. Failure to following these instructions will result in an individual\u2019s pcard privileges discontinued.","title":"Genaral procedure"},{"location":"admin/pcard/#details","text":"Obtaining the Pcard for Use: - Over the Phone Contact Erma Santander at 520-621-7120, Monday thru Thursday from 10am \u2013 4pm, to obtain pcard information and a log number for a one-time use. If at one time you were given the pcard number to purchase an item, you are not authorized to keep this number written down or to use it multiple times. - Physical Card Two department pcards are available for checkout Monday thru Thursday from 10am -4pm in Harshbarger Room 202, with a maximum checkout time of no more than two hours. No overnight or weekend checkouts are permitted. When a pcard is checked out you will receive a log number for its one-time use.","title":"Details"},{"location":"admin/travel_autorization/","text":"Information Regarding Travel Authorization A Travel Authorization is required when a University of Arizona Employee, Student, or Designated Campus Colleague will be on travel status (at least 35 miles outside of their duty post) and travels on behalf of University business, regardless of expenses incurred by the University. The form must be downloaded always from https://financialservices.arizona.edu/form/travel-authorization-form because it has a unique correlative number. You must save the form into your device before filling it out. If you complete the form in your web browser data will be lost. Tips The UA NETID number can be found at the top right of the uaccess web page. The DEPT/ORG code is HWRS/469. If you have an RA you are considered an employee. In other cases, you are just a student. For the funding source (Account), in the travel detail section, you should ask Garry gforger@arizona.edu about your account. Ask every time because your funding source could change. International travel requires an additional form. This form can takes weeks to be approved so do not leave it to the last day. The approver, in the Travel Authorization and Funding Approval section, is Cindy Barnett barnettc@arizona.edu . You can check with Garry because the approver could change.","title":"Information Regarding Travel Authorization"},{"location":"admin/travel_autorization/#information-regarding-travel-authorization","text":"A Travel Authorization is required when a University of Arizona Employee, Student, or Designated Campus Colleague will be on travel status (at least 35 miles outside of their duty post) and travels on behalf of University business, regardless of expenses incurred by the University. The form must be downloaded always from https://financialservices.arizona.edu/form/travel-authorization-form because it has a unique correlative number. You must save the form into your device before filling it out. If you complete the form in your web browser data will be lost.","title":"Information Regarding Travel Authorization"},{"location":"admin/travel_autorization/#tips","text":"The UA NETID number can be found at the top right of the uaccess web page. The DEPT/ORG code is HWRS/469. If you have an RA you are considered an employee. In other cases, you are just a student. For the funding source (Account), in the travel detail section, you should ask Garry gforger@arizona.edu about your account. Ask every time because your funding source could change. International travel requires an additional form. This form can takes weeks to be approved so do not leave it to the last day. The approver, in the Travel Authorization and Funding Approval section, is Cindy Barnett barnettc@arizona.edu . You can check with Garry because the approver could change.","title":"Tips"},{"location":"admin/travel_reimbursement/","text":"Information Regarding Reimbursement for Travel General comments You always need a signed travel authorization form before you submit a travel reimbursement. For any event you are attending you can submit for up to two reimbursements (CHECK THAT WITH CINDY). For something like AGU one good way to do this is to submit once for your plane ticket and registration before you go and once after for hotels food etc (Make sure you save receipts for everything in order to get reimbursed (see note below on food). You can be reimbursed for any expenses associated with your travel to the conference (airplane tickets, ground transportation, airport parking, hotels, and food). For transportation like Uber or Lyfts, we need the detailed receipt that shows how much everything was charged and it includes the map. Remember ANY tips are approved up to 20%, not a penny more. You can't be reimbursed for alcohol or expenses associated with any entertainment or outings you do for fun while attending a conference. The goal is for you to be fully reimbursed for your travel expenses so it is not a personal expense to you. However, please remember that your travel is supported by the grants and we want to be able to support as many people attending conferences as possible so you should be reasonable with your spending. This means that students are expected to share a room unless it is not feasible or would put you in a position you would not feel comfortable (you should get permission from Laura in advance if this is the case). This also means that you should not request the full per diem allotment for food every day. Rather please add up all your food expenses at the end and request the correct number of lunches/dinners/breakfasts to make sure that you are compensated for what you spend but not making a profit. (The alternative is you submit receipts for every meal... I find this to be very challenging though especially when eating with other people and sharing checks so I think the best option is just to keep track of your spending and then request per diem to match) General Per Diem Information for UA - https://policy.fso.arizona.edu/fsm/1400/1413 After you return you will need to submit a reimbursement form to Cindy Barnett barnettc@arizona.edu with all your receipts. Tips The form must be downloaded always from https://financialservices.arizona.edu/form/travel-expense-report The travel authorization on the top right must match the signed travel authorization. The form should sum automatically but it almost never works so be prepared to fill and sign it manually. It should calculate if you are using chrome, not Edge. It is possible that additional questions will be required about the purpose of the trip. Do not worry...It is just bureaucracy. The entire process takes a couple of months, so you can ask Cindy about the status of reimbursement.","title":"Information Regarding Reimbursement for Travel"},{"location":"admin/travel_reimbursement/#information-regarding-reimbursement-for-travel","text":"","title":"Information Regarding Reimbursement for Travel"},{"location":"admin/travel_reimbursement/#general-comments","text":"You always need a signed travel authorization form before you submit a travel reimbursement. For any event you are attending you can submit for up to two reimbursements (CHECK THAT WITH CINDY). For something like AGU one good way to do this is to submit once for your plane ticket and registration before you go and once after for hotels food etc (Make sure you save receipts for everything in order to get reimbursed (see note below on food). You can be reimbursed for any expenses associated with your travel to the conference (airplane tickets, ground transportation, airport parking, hotels, and food). For transportation like Uber or Lyfts, we need the detailed receipt that shows how much everything was charged and it includes the map. Remember ANY tips are approved up to 20%, not a penny more. You can't be reimbursed for alcohol or expenses associated with any entertainment or outings you do for fun while attending a conference. The goal is for you to be fully reimbursed for your travel expenses so it is not a personal expense to you. However, please remember that your travel is supported by the grants and we want to be able to support as many people attending conferences as possible so you should be reasonable with your spending. This means that students are expected to share a room unless it is not feasible or would put you in a position you would not feel comfortable (you should get permission from Laura in advance if this is the case). This also means that you should not request the full per diem allotment for food every day. Rather please add up all your food expenses at the end and request the correct number of lunches/dinners/breakfasts to make sure that you are compensated for what you spend but not making a profit. (The alternative is you submit receipts for every meal... I find this to be very challenging though especially when eating with other people and sharing checks so I think the best option is just to keep track of your spending and then request per diem to match) General Per Diem Information for UA - https://policy.fso.arizona.edu/fsm/1400/1413 After you return you will need to submit a reimbursement form to Cindy Barnett barnettc@arizona.edu with all your receipts.","title":"General comments"},{"location":"admin/travel_reimbursement/#tips","text":"The form must be downloaded always from https://financialservices.arizona.edu/form/travel-expense-report The travel authorization on the top right must match the signed travel authorization. The form should sum automatically but it almost never works so be prepared to fill and sign it manually. It should calculate if you are using chrome, not Edge. It is possible that additional questions will be required about the purpose of the trip. Do not worry...It is just bureaucracy. The entire process takes a couple of months, so you can ask Cindy about the status of reimbursement.","title":"Tips"},{"location":"example_scripts/","text":"Example scripts Example script for a simple hello world script in python Hello World Jupyter Notebook showing the storage-elevation changes for Lake Powell using USGS and ResOps data Jupyter Notebook showing how to interact with Google drive via a notebook and a new way to analyze Parflow outputs using xarray","title":"Example scripts"},{"location":"example_scripts/#example-scripts","text":"Example script for a simple hello world script in python Hello World Jupyter Notebook showing the storage-elevation changes for Lake Powell using USGS and ResOps data Jupyter Notebook showing how to interact with Google drive via a notebook and a new way to analyze Parflow outputs using xarray","title":"Example scripts"},{"location":"group_meetings/","text":"Group Meetings Below is a summary of our past group meetings. They are organized by year and semester. After the conclusion of each semester, the lab group meeting coordinator will update this doc accordingly. Make sure to change the permissions on any slide links to viewer only. 2023 Spring Group Meeting Coordinator: Patricia Puente Date Description Resources 2/2 Two Slide Research Updates and Discussion on the Salton Sea Slides for the Salton sea 2/9 Laura Practice Presentation 2/23 One Slide Research Updates 3/2 Planet Data Product and Resources with Patricia Puente Slides for Planet Demonstration 3/16 Planet APIs and Basemaps demo with Daniel Luna Slides for Planet API Demonstration 4/6 One Slide Research Updates 5/11 Building a Website with Patricia Puente Slides for Website Building Fall Group Meeting Coordinator: Danielle Tadych Date Description Resources 9/7 Discussed an article on declining global water storage Satellites reveal widespread decline in global lake water storage 9/21 Writing Assignment 9/28 Debugging how-to's, best practices, and resources with Ben West Link for Debugging Slides 10/12 Website Workshopping Day with Patricia 10/19 Picture Day and Paper Discussion of an article about ChatGPT and its applications to the Natural Sciences ChatGPT in Hydrology and Earth Sciences: Opportunities, Prospects, and Concerns 10/26 Research Updates 11/9 How Read the Docs works and division of tasks with Will and Laura 11/16 Author of Brave the Wild River, Melissa Vesigny, Q&A","title":"Group Meetings"},{"location":"group_meetings/#group-meetings","text":"Below is a summary of our past group meetings. They are organized by year and semester. After the conclusion of each semester, the lab group meeting coordinator will update this doc accordingly. Make sure to change the permissions on any slide links to viewer only.","title":"Group Meetings"},{"location":"group_meetings/#2023","text":"","title":"2023"},{"location":"group_meetings/#spring","text":"Group Meeting Coordinator: Patricia Puente Date Description Resources 2/2 Two Slide Research Updates and Discussion on the Salton Sea Slides for the Salton sea 2/9 Laura Practice Presentation 2/23 One Slide Research Updates 3/2 Planet Data Product and Resources with Patricia Puente Slides for Planet Demonstration 3/16 Planet APIs and Basemaps demo with Daniel Luna Slides for Planet API Demonstration 4/6 One Slide Research Updates 5/11 Building a Website with Patricia Puente Slides for Website Building","title":"Spring"},{"location":"group_meetings/#fall","text":"Group Meeting Coordinator: Danielle Tadych Date Description Resources 9/7 Discussed an article on declining global water storage Satellites reveal widespread decline in global lake water storage 9/21 Writing Assignment 9/28 Debugging how-to's, best practices, and resources with Ben West Link for Debugging Slides 10/12 Website Workshopping Day with Patricia 10/19 Picture Day and Paper Discussion of an article about ChatGPT and its applications to the Natural Sciences ChatGPT in Hydrology and Earth Sciences: Opportunities, Prospects, and Concerns 10/26 Research Updates 11/9 How Read the Docs works and division of tasks with Will and Laura 11/16 Author of Brave the Wild River, Melissa Vesigny, Q&A","title":"Fall"},{"location":"how_to_notes/HPC_systems/UA_HPC/","text":"UA HPC Information Laura Condon, Oct, 2018 (Updated) Quinn Hull, Nov 2020 (Updated) Rachel Spinti, Feb 2021 Description Running notes with links and tips for running on the UA HPC system. Feel free to update and add to this document as needed. Ocelote Quick Start A good starting point. Tutorial highlights essential steps in running a basic job on HPC Ocelote. How to log in What a login node is What a job scheduler is How to access software How to run a job on HPC https://public.confluence.arizona.edu/display/UAHPC/Ocelote+Quick+Start Puma Quick Start: Tutorial highlights essential steps in running a basic job on HPC Puma. How to log in What a login node is How to access software (Note: it is different than Ocelote, see https://public.confluence.arizona.edu/display/UAHPC/Puma+Quick+Start#PumaQuickStart-AccessingSoftware ) How to write a SLURM script How to run a job on Puma https://public.confluence.arizona.edu/display/UAHPC/Puma+Quick+Start UITS Account Management To set-up a UA HPC account if you have not activated one. Sign-in and go to \u201cManage Your Accounts\u201d https://account.arizona.edu/welcome HPC portal https://portal.hpc.arizona.edu Logging in _ssh username@hpc.arizona.edu (e.g. roberthull@hpc.arizona.edu)_ Should give you a choice to connect to Ocelote - if not you can type menuon to get the menu Type ocelote to enter the real HPC system HPC Browser Dashboard (OnDemand) https://ood.hpc.arizona.edu/pun/sys/dashboard A GUI that allows you to monitor jobs and access files outside the terminal. Can be easier to view and/or edit files this way than in command line. Making a run script To execute a \u2018job\u2019 (ex: execute tclsh script) on HPC, one must \u2018submit\u2019 a request using the scheduling software PBS. https://public.confluence.arizona.edu/pages/viewpage.action?pageId=86409309 PBS Overview: Jobs are submitted to the batch system using PBS scripts that specify the job's required resources such as number of nodes, cpus, memory, group, wallclock time. https://jobbuilder.hpc.arizona.edu// This site is helpful to start, but is a bit outdated: see the example pbs script at the end of this doc for more detail https://public.confluence.arizona.edu/display/UAHPC/Ocelote+Quick+Start A (very helpful) \u201chello world\u201d tutorial including job submission via pbs script Link for computing account management https://account.arizona.edu/welcome Instructions for setting up and account https://public.confluence.arizona.edu/display/UAHPC/Account+Creation Accessing Software via Module commands https://docs.hpc.arizona.edu/display/UAHPC/Accessing+Software Module list (gives loaded modules) Module avail (gives list of all modules) Module show NAME (gives info on a specific module Storage and limits https://docs.hpc.arizona.edu/display/UAHPC/Allocation+and+Limits 50GB on home/uxx/netid (ex home/u8/roberthull) 200GB (no Backups) on extra/netid /temp - 840GB available per node. You can use this during the job and then do a final write to a shared array. (You cannot permanently save anything to this location) No more than 600files/GB ~1.6 MB per file There is a total time limitation for our group (36,000 hours/month), use command va to check the remaining time of the month. Uploading files to UAHPC: https://public.confluence.arizona.edu/display/UAHPC/Transferring+Files (small files) Use the GUI web terminal https://ood.hpc.arizona.edu/pun/sys/dashboard Log on Click \u2018Files\u2019 in top left corner Select Directory to which you want to add files (e.g. Click Upload and navigate to the local file location. To use the file, navigate to the file location by clicking Open in terminal (big files <100 GB). Use sftp, scp or rsync using filexfer.hpc.arizona.edu Copy your files through through the transfer node like below: _scp -rp file.txt &lt;netid>[@filexfer.hpc.arizona.edu](mailto:lecondon@filexfer.hpc.arizona.edu):&lt;directory>_ Example: If I (roberthull) want to upload a python script (test.py) to my home directory (/home/u8/roberthull) the command would look like the below _scp -rp test.py roberthull[@filexfer.hpc.arizona.edu](mailto:lecondon@filexfer.hpc.arizona.edu):_/home/u8/roberthull *** Remember that on all HPC systems your files will be purged after a certain amount of time it is your job to be aware of the purge policies and copy your outputs somewhere appropriate for longer term storage. *** Note as shown above scp requires http://filexfer.hpc.arizona.edu/ rather than hpc.arizona.edu Monitoring jobs [https://public.confluence.arizona.edu/pages/viewpage.action?pageId=86409309](https://public.confluence.arizona.edu/pages/viewpage.action?pageId=86409309 *Note qstat -u will truncate the job_id to get it not to do this you need to use qstat -antsw1u lecondon Getting started Steps for getting started and running your first job: Get an account see instructions above Login ssh username@hpc.arizona.edu - _remember when you first login you need to type the correct number to actually get into Ocelote Build ParFlow on Ocelote (follow the instructions below) Upload your files to the directory you want to run in. Look at the storage limits links above to learn about the places you can store files. Your home directory is VERY small 50GB so you don\u2019t want to keep to much here but it\u2019s a good place to put things that you don\u2019t want to be deleted. The \u2018extra\u2019 directory has more space and is a better place to run. You can copy your files using the scp command through the filexfer node like this: scp -rp file.txt [lecondon@filexfer.hpc.arizona.edu](mailto:lecondon@filexfer.hpc.arizona.edu):/extra/lecondon/wash_base *** **Remember that on all HPC systems your files will be purged after a certain amount of time it is your job to be aware of the purge policies and copy your outputs somewhere appropriate for longer term storage. ** *** Note as shown above scp requires http://filexfer.hpc.arizona.edu/ rather than hpc.arizona.edu Build a run script. You can\u2019t just tclsh on a HPC machine you need to \u2018submit\u2019 your job to request time and specify how much resources you will need. See the example run scripts below and the link above for generating a run script. Note: There are different queues that you can submit to. If you just want to test something quickly the debug queue is a good option. You can read about the different queues in the storage and limits section and see an example of a debug submission below Submit your job to the queue by running the qsub command on the run script you just created. qsub myrunscrp.pbs Minitor your job. You can see when your job starts running and finishes using the commands in the run monitoring section. Copy your outputs back to your local machine using scp commands like in step four or upload to cyverse using irods commands. ParFlow on UAHPC Running with an existing version of ParFlow _If you want to run with one of the versions of ParFlow that has already been build just add these lines to your bashrc and make sure that you aren\u2019t also setting \u2018PARFLOW_DIR\u2019 in in your bashrc file. _ ssh [username@hpc.arizona.edu](mailto:username@hpc.arizona.edu) cd $HOME vi ~/.bashrc # Add the following lines to this file module load unsupported module load lecondon/parflow/latest # You can check if things look right using the commands which parflow echo $PARFLOW_DIR Adding a new ParFlow Module Refer to the README file in: /unsupported/lecondon/parflow/ These are the old instructions for building ParFlow with Autoconf. Ignore these and use the CMAKE ones Download ParFlow git clone https://github.com/parflow/parflow Rename this directory to whatever you want Setup environment vi ~/.bashrc Add the following lines to the user specified portion: module load gcc module load hypre module load silo export PARFLOW_DIR=/home/PATH_TO_YOUR_PARFLOW_DIR PATH=$PATH:$PARFLOW_DIR/bin source ~/.bashrc Build ParFlow cd pfsimulator ./configure --prefix=$PARFLOW_DIR --with-amps=mpi1 --with-clm --with-hypre=$HYPRE_BASE --with-silo=$SILO_BASE --with-amps-sequential-io make -j 14_ _make install_ _Build PFtools_ _cd ../pftools_ _./configure --prefix=$PARFLOW_DIR --with-amps=mpi1 --with-clm --with-hypre=$HYPRE_BASE --with-silo=$SILO_BASE --with-amps-sequential-io_ make -j 14 make install Run tests cd ../test make check Example pbs script This is an example of what a job submit script should look like. You can generate one based off of this or using the link above # Your job will use 1 node, 8 cores, and 48gb of memory total. #PBS -q standard #PBS -l select=1:ncpus=16:mem=48gb:pcmem=6gb ### Specify a name for the job #PBS -N test ### Specify the group name #PBS -W group_list=lecondon ### Walltime is how long your job will run #PBS -l walltime=00:50:00 ### Joins standard error and standard out #PBS -j oe cd /home/u18/lecondon/Test/washita/tcl_scripts tclsh Dist_Forcings.tcl tclsh LW_Test.tcl Note: the \u201cplace=pack:shared\u201d and \u201ccput\u201d lines in the HPC job builder guide website are no longer necessary as of July 2019. SLURM scripts Puma uses SLURM to submit jobs instead of PBS. The link above has a PBS to SLURM Rosetta stone, so you can convert your PBS script. Otherwise, the Puma Quick start guide contains a sample SLURM script ( https://public.confluence.arizona.edu/display/UAHPC/Puma+Quick+Start#PumaQuickStart-WritingaSLURMSubmissionScript) ). Debug queue The debug queue is a high-priority queue with a short runtime that is used to debug your runs before sending them to the standard or other queue. More information can be found here: https://docs.hpc.arizona.edu/display/UAHPC/Allocation+and+Limits Example pbs script for debug queue (comments are in {} and not to be included in script): # Your job will use 1 node, 8 cores, and 48gb of memory total. #PBS -q debug {changing specified queue to the debug queue} #PBS -l select=1:ncpus=16:mem=48gb:pcmem=6gb {cannot exceed 2 nodes or 56 total cores} ### Specify a name for the job #PBS -N test ### Specify the group name #PBS -W group_list=lecondon ### Walltime is how long your job will run #PBS -l walltime=00:10:00 {walltime cannot exceed 10 minutes} ### Joins standard error and standard out #PBS -j oe Once the job has been submitted to the debug queue and been executed, the error and output messages for the job will indicate whether the job ran for the total allotted time or ran into an error before then. Time Allocation Each PI has a finite amount of time allocation each month. After each run completes, the _run_name.o####### _file will show the amount of time left in the standard queue each month. Each time you submit a script, the system makes sure that your request can be fulfilled in the given queue - otherwise, it returns an error message. If you\u2019re requesting more time than you have for the rest of the month, you can: 1) reduce the amount of time you are requesting, 2) submit it to the windfall queue, or 3) wait until the end of the month and try again next month. Your advisor may not be okay with you trying option 3. Accessing and using software (like Python) General: https://public.confluence.arizona.edu/display/UAHPC/Accessing+Software Python: https://public.confluence.arizona.edu/display/UAHPC/Using+and+Installing+Python Conda: https://public.confluence.arizona.edu/display/UAHPC/Open+On+Demand Summary: Many software are installed on the server (> 100). Note that both Python and Anaconda exist, which allow for the installation of Python packages and usage of conda virtual environments. Python: There are four versions of Python on Ocelote, (up to Python 3.6.5). Python 3.6.5 is loaded with \u2018module load python\u2019, and includes machine learning packages like Tensorflow. (Does it include PyTorch?) There are three versions of Python in ElGato (up to Python 3.8.0). Python 3.8.0 is loaded with \u2018module load python/3.8/3.8.0\u2019 There are two versions of Python in Puma (up to Python 3.8.2) Note that for DL software, Puma is best https://public.confluence.arizona.edu/display/UAHPC/Puma+Quick+Start Virtual Environments It is useful to set up a virtualenv within your working directory. Conda Anaconda is available as a module, with its requisite packages and access to virtual environments (too?) Look at Conda Environments: conda env list Create Conda Environments: conda create --name -myenv Activate Conda Environments: conda activate and conda deactivate How to do a basic script in Python Log in via terminal or bash see the \"Logging In\" section Find Python modules module avail python This will list availalbe python modules. e.g. python/3.6/3.65 Load Python module load python/3.6/3.65 Add/Create/Locate the script you want to run pytorch_helloworld.py -> print(\"hello world\") Create and edit pbs script (note important fields) touch pytorch_helloworld.pbs vim pytorch_helloworld.pbs # -------------------------------------------------------------- ### PART 1: Requests resources to run your job. # -------------------------------------------------------------- ### Optional. Set the job name #PBS -N pytorch_helloworld ### REQUIRED. Specify the PI group for this job #PBS -W group_list=lecondon ### Optional. Request email when job begins and ends ### PBS -m bea ### Optional. Specify email address to use for notification ### PBS -M roberthull@email.arizona.edu ### REQUIRED. Set the queue for your job. #PBS -q windfall ### REQUIRED. Set the number of nodes, cores and memory that will be used for this job ### pcmem is optional as it defaults to 6gb per core. Only required for high memory = 42gb. #PBS -l select=1:ncpus=1:mem=6gb:pcmem=6gb ### REQUIRED. Specify \"wallclock time\" required for this job, hhh:mm:ss #PBS -l walltime=0:1:0 ### Optional. cput = time x ncpus. Default: cput = walltime x ncpus. #PBS -l cput=0:1:0 # -------------------------------------------------------------- ### PART 2: Executes bash commands to run your job # -------------------------------------------------------------- ### Load required modules/libraries if needed module load python/3.6/3.6.5 ### change to your scripts directory cd ~/python ### run your work python3 pytorch_helloworld.py sleep 10i Submit job via qsub qsub pytorch_helloworld.pbs3592178.head1.cm.cluster Follow job progression via qstat View results of job <jobname>.o<number> for results, ` .e for errors Look at results: vim pytorch_helloworld.o3692196 -> hello world If there are errors use: vim pytorch_helloworld.e3692196 How to run a more complicated Python script in terminal on UAHPC Log in via terminal or bash (go to Ocelote) See the \"Logging in\" section Log in via terminal or bash see the \"Logging In\" section Find Python modules module avail python This will list availalbe python modules. e.g. python/3.6/3.65 Load Python module load python/3.6/3.65 Take care that you load the version of python that you need. That includes considerations of what packages are available via UAHPC, and how you\u2019ve installed custom packages in your virtual environment Also, you can check what python versions are available and/or are loaded using module list Activate your virtual environment (create it if necessary) virtualenv --system-site-packages ~/mypyenv_oc_py.3.6 source ~/mypyenv_oc_py3.6/bin/activate Take care to use a virtual environment compatible with your version of python and with your super computer of choice (Ocelote v ElGato) Add/Create/Locate the script you want to run Pytorch_gpus.py \u2192 runs a simple gpu script Note if you use custom packages you will need to manually reference the location of your virtual environment in the following way within your python script: Check which packages you have in your virtual environment, pip list (note that python/3.6/3.6.5 for ocelote has almost everything you would ever want to have\u2026) Install any packages that you think you need to run your desired script pip install <name_of_package> Create and edit pbs script (note important fields) #!/bin/bash # -------------------------------------------------------------- ### PART 1: Requests resources to run your job. # -------------------------------------------------------------- ### Optinal. Set the job name #PBS -N pytorch_gpus_n1 ### REQUIRED. Specify the PI group for this job #PBS -W group_list=lecondon ### Optional. Request email when job begins and ends ### PBS -m bea ### Optional. Specify email address to use for notification ### PBS -M &lt;YOUR NETID>@email.arizona.edu ### REQUIRED. Set the queue for your job. #PBS -q windfall ### REQUIRED. Set the number of nodes, cores and memory that will be used for this job ### pcmem is optional as it defaults to 6gb per core. Only required for high memory = 42gb. #PBS -l select=1:ncpus=1:mem=6gb ### REQUIRED. Specify \"wallclock time\" required for this job, hhh:mm:ss #PBS -l walltime=0:1:0 ### Optional. cput = time x ncpus. Default: cput = walltime x ncpus. #PBS -l cput=0:1:0 # -------------------------------------------------------------- ### PART 2: Executes bash commands to run your job # -------------------------------------------------------------- ### Load required modules/libraries if needed module load python/3.6/3.6.5 #### Activate virtual environment source ~/mypyenv_oc_py3.6/bin/activate ### change to your script\u2019s directory cd ~/python ### Run your work python3 pytorch_gpus.py sleep 10 A quick note about Requesting Resources and Node Types (Standard, GPU, or High Memory) via the select statement. The basic `select statement is: #PBS -l select=x:ncpus=Y:mem=Zgb Where: x = The number of nodes or units of the resources required Y = The number of cores (individual processors) required on each node Z = The amount of memory (in mb or gb) required on each node \u2018Normal\u2019 Requests Ocelote For Ocelote all of the standard nodes have 28 cores and 6GB per core, pcmem=6gb can be added to the line or left off, and it will default to 6gm. The following select statement would request one complete node: #PBS -l select=28:ncpus=28:pcmem=6gb This is an example of a job that uses two nodes #PBS -l select=2:ncpus=28:mem=16gb El Gato On El Gato all of the standard nodes of 16 cores and 4GB per core. The maximum available memory on a standard El Gato node is 62GB, leaving the differences for the operating system. The following select statement would request one complete node #PBS -l select=1:ncpus=16:pcmem=4gb \u2018GPU\u2019 Requests Ocelote On Ocelote there are 46 Nvidia P100 nodes available with 28 cores and 224 GB of memory. Users of these nodes can use either their standard allocation of cpu hours or windfall time. Jobs that do not need GPU's will not run on them, including windfall jobs taht do no need the GPU. EAch job will have exclusive access to the node to prevent contention between the CPU cores and the GPU. Each group is limited to ten GPU nodes concurrently. This requests a CentOs 7 GPU node: #PBS -l select=1:ncpus=28:mem=224gb:np100s=1:os7=True or #PBS -l select=1:ncpus=28:mem=224gb:ngpus=1:os7=True This requests a CentOS 6 GPU node: #PBS -l select=1:ncpus=28:mem=224gb:np100s=1 Ocelote has a single node with two GPUs that may be requested with #PBS -l select=1:ncpus=28:mem=224gb:np100s=2 El Gato Unlike Ocelote, El Gato has cgroups enabled which allows for selecting a partial GPU node. Memory requests should be scaled by ncpusx16gb. To request a full GPU node: #PBS -l select=1:ncpus=16:mem=250gb:ngpus=1:pcmem=16gb To request a single node with two GPUs: #PBS -l select=1:ncpus=16:mem=250gb:ngpus=2:pcmem=16gb An example of a GPU pbs using pytorch is as follows #!/bin/bash # -------------------------------------------------------------- ### PART 1: Requests resources to run your job. # -------------------------------------------------------------- #PBS -q standard #PBS -l select=1:ncpus=28:mem=168gb:pcmem=6gb:np100s=1:os7=True ### Specify a name for the job #PBS -N pytorch_gpus_single3 ### Specify the group name #PBS -W group_list=lecondon ### Walltime is how long your job will run #PBS -l walltime=00:05:00 # -------------------------------------------------------------- ### PART 2: Executes bash commands to run your job # -------------------------------------------------------------- ### Load required modules/libraries if needed module load python/3.6/3.6.5 ### In case needed for gpu..? module load pytorch/nvidia/20.01 ### Activate virtual environment source ~/mypyenv_oc_py3.6/bin/activate ### change to your script\u2019s directory cd ~/python ### Run your work singularity exec --nv /cm/shared/uaapps/pytorch/20.01/nvidia-pytorch.20.01-py3.simg python pytorch_gpus.py sleep 10 Submit job via qsub qsub pytorch_gpus_n1.pbs2708168.head1.cm.cluster Follow job progression via qstat qstat pbs2708168 View results of job <jobname>.o<number> for results, <jobname>.e<number> for errors How to run Python in a Jupiter notebook on UAHPC It might be easiest to run Python using a Jupyter Notebook, for which UAHPC has developed super convenient GUIs. Check here for a great tutorial.","title":"UA HPC Information"},{"location":"how_to_notes/HPC_systems/UA_HPC/#ua-hpc-information","text":"Laura Condon, Oct, 2018 (Updated) Quinn Hull, Nov 2020 (Updated) Rachel Spinti, Feb 2021","title":"UA HPC Information"},{"location":"how_to_notes/HPC_systems/UA_HPC/#description","text":"Running notes with links and tips for running on the UA HPC system. Feel free to update and add to this document as needed.","title":"Description"},{"location":"how_to_notes/HPC_systems/UA_HPC/#ocelote-quick-start","text":"A good starting point. Tutorial highlights essential steps in running a basic job on HPC Ocelote. How to log in What a login node is What a job scheduler is How to access software How to run a job on HPC https://public.confluence.arizona.edu/display/UAHPC/Ocelote+Quick+Start","title":"Ocelote Quick Start"},{"location":"how_to_notes/HPC_systems/UA_HPC/#puma-quick-start","text":"Tutorial highlights essential steps in running a basic job on HPC Puma. How to log in What a login node is How to access software (Note: it is different than Ocelote, see https://public.confluence.arizona.edu/display/UAHPC/Puma+Quick+Start#PumaQuickStart-AccessingSoftware ) How to write a SLURM script How to run a job on Puma https://public.confluence.arizona.edu/display/UAHPC/Puma+Quick+Start","title":"Puma Quick Start:"},{"location":"how_to_notes/HPC_systems/UA_HPC/#uits-account-management","text":"To set-up a UA HPC account if you have not activated one. Sign-in and go to \u201cManage Your Accounts\u201d https://account.arizona.edu/welcome","title":"UITS Account Management"},{"location":"how_to_notes/HPC_systems/UA_HPC/#hpc-portal","text":"https://portal.hpc.arizona.edu","title":"HPC portal"},{"location":"how_to_notes/HPC_systems/UA_HPC/#logging-in","text":"_ssh username@hpc.arizona.edu (e.g. roberthull@hpc.arizona.edu)_ Should give you a choice to connect to Ocelote - if not you can type menuon to get the menu Type ocelote to enter the real HPC system","title":"Logging in"},{"location":"how_to_notes/HPC_systems/UA_HPC/#hpc-browser-dashboard-ondemand","text":"https://ood.hpc.arizona.edu/pun/sys/dashboard A GUI that allows you to monitor jobs and access files outside the terminal. Can be easier to view and/or edit files this way than in command line.","title":"HPC Browser Dashboard (OnDemand)"},{"location":"how_to_notes/HPC_systems/UA_HPC/#making-a-run-script","text":"To execute a \u2018job\u2019 (ex: execute tclsh script) on HPC, one must \u2018submit\u2019 a request using the scheduling software PBS. https://public.confluence.arizona.edu/pages/viewpage.action?pageId=86409309 PBS Overview: Jobs are submitted to the batch system using PBS scripts that specify the job's required resources such as number of nodes, cpus, memory, group, wallclock time. https://jobbuilder.hpc.arizona.edu// This site is helpful to start, but is a bit outdated: see the example pbs script at the end of this doc for more detail https://public.confluence.arizona.edu/display/UAHPC/Ocelote+Quick+Start A (very helpful) \u201chello world\u201d tutorial including job submission via pbs script","title":"Making a run script"},{"location":"how_to_notes/HPC_systems/UA_HPC/#link-for-computing-account-management","text":"https://account.arizona.edu/welcome","title":"Link for computing account management"},{"location":"how_to_notes/HPC_systems/UA_HPC/#instructions-for-setting-up-and-account","text":"https://public.confluence.arizona.edu/display/UAHPC/Account+Creation","title":"Instructions for setting up and account"},{"location":"how_to_notes/HPC_systems/UA_HPC/#accessing-software-via-module-commands","text":"https://docs.hpc.arizona.edu/display/UAHPC/Accessing+Software Module list (gives loaded modules) Module avail (gives list of all modules) Module show NAME (gives info on a specific module","title":"Accessing Software via Module commands"},{"location":"how_to_notes/HPC_systems/UA_HPC/#storage-and-limits","text":"https://docs.hpc.arizona.edu/display/UAHPC/Allocation+and+Limits 50GB on home/uxx/netid (ex home/u8/roberthull) 200GB (no Backups) on extra/netid /temp - 840GB available per node. You can use this during the job and then do a final write to a shared array. (You cannot permanently save anything to this location) No more than 600files/GB ~1.6 MB per file There is a total time limitation for our group (36,000 hours/month), use command va to check the remaining time of the month.","title":"Storage and limits"},{"location":"how_to_notes/HPC_systems/UA_HPC/#uploading-files-to-uahpc","text":"https://public.confluence.arizona.edu/display/UAHPC/Transferring+Files (small files) Use the GUI web terminal https://ood.hpc.arizona.edu/pun/sys/dashboard Log on Click \u2018Files\u2019 in top left corner Select Directory to which you want to add files (e.g. Click Upload and navigate to the local file location. To use the file, navigate to the file location by clicking Open in terminal (big files <100 GB). Use sftp, scp or rsync using filexfer.hpc.arizona.edu Copy your files through through the transfer node like below: _scp -rp file.txt &lt;netid>[@filexfer.hpc.arizona.edu](mailto:lecondon@filexfer.hpc.arizona.edu):&lt;directory>_ Example: If I (roberthull) want to upload a python script (test.py) to my home directory (/home/u8/roberthull) the command would look like the below _scp -rp test.py roberthull[@filexfer.hpc.arizona.edu](mailto:lecondon@filexfer.hpc.arizona.edu):_/home/u8/roberthull *** Remember that on all HPC systems your files will be purged after a certain amount of time it is your job to be aware of the purge policies and copy your outputs somewhere appropriate for longer term storage. *** Note as shown above scp requires http://filexfer.hpc.arizona.edu/ rather than hpc.arizona.edu","title":"Uploading files to UAHPC:"},{"location":"how_to_notes/HPC_systems/UA_HPC/#monitoring-jobs","text":"[https://public.confluence.arizona.edu/pages/viewpage.action?pageId=86409309](https://public.confluence.arizona.edu/pages/viewpage.action?pageId=86409309 *Note qstat -u will truncate the job_id to get it not to do this you need to use qstat -antsw1u lecondon","title":"Monitoring jobs"},{"location":"how_to_notes/HPC_systems/UA_HPC/#getting-started","text":"Steps for getting started and running your first job: Get an account see instructions above Login ssh username@hpc.arizona.edu - _remember when you first login you need to type the correct number to actually get into Ocelote Build ParFlow on Ocelote (follow the instructions below) Upload your files to the directory you want to run in. Look at the storage limits links above to learn about the places you can store files. Your home directory is VERY small 50GB so you don\u2019t want to keep to much here but it\u2019s a good place to put things that you don\u2019t want to be deleted. The \u2018extra\u2019 directory has more space and is a better place to run. You can copy your files using the scp command through the filexfer node like this: scp -rp file.txt [lecondon@filexfer.hpc.arizona.edu](mailto:lecondon@filexfer.hpc.arizona.edu):/extra/lecondon/wash_base *** **Remember that on all HPC systems your files will be purged after a certain amount of time it is your job to be aware of the purge policies and copy your outputs somewhere appropriate for longer term storage. ** *** Note as shown above scp requires http://filexfer.hpc.arizona.edu/ rather than hpc.arizona.edu Build a run script. You can\u2019t just tclsh on a HPC machine you need to \u2018submit\u2019 your job to request time and specify how much resources you will need. See the example run scripts below and the link above for generating a run script. Note: There are different queues that you can submit to. If you just want to test something quickly the debug queue is a good option. You can read about the different queues in the storage and limits section and see an example of a debug submission below Submit your job to the queue by running the qsub command on the run script you just created. qsub myrunscrp.pbs Minitor your job. You can see when your job starts running and finishes using the commands in the run monitoring section. Copy your outputs back to your local machine using scp commands like in step four or upload to cyverse using irods commands.","title":"Getting started"},{"location":"how_to_notes/HPC_systems/UA_HPC/#parflow-on-uahpc","text":"Running with an existing version of ParFlow _If you want to run with one of the versions of ParFlow that has already been build just add these lines to your bashrc and make sure that you aren\u2019t also setting \u2018PARFLOW_DIR\u2019 in in your bashrc file. _ ssh [username@hpc.arizona.edu](mailto:username@hpc.arizona.edu) cd $HOME vi ~/.bashrc # Add the following lines to this file module load unsupported module load lecondon/parflow/latest # You can check if things look right using the commands which parflow echo $PARFLOW_DIR Adding a new ParFlow Module Refer to the README file in: /unsupported/lecondon/parflow/ These are the old instructions for building ParFlow with Autoconf. Ignore these and use the CMAKE ones Download ParFlow git clone https://github.com/parflow/parflow Rename this directory to whatever you want Setup environment vi ~/.bashrc Add the following lines to the user specified portion: module load gcc module load hypre module load silo export PARFLOW_DIR=/home/PATH_TO_YOUR_PARFLOW_DIR PATH=$PATH:$PARFLOW_DIR/bin source ~/.bashrc Build ParFlow cd pfsimulator ./configure --prefix=$PARFLOW_DIR --with-amps=mpi1 --with-clm --with-hypre=$HYPRE_BASE --with-silo=$SILO_BASE --with-amps-sequential-io make -j 14_ _make install_ _Build PFtools_ _cd ../pftools_ _./configure --prefix=$PARFLOW_DIR --with-amps=mpi1 --with-clm --with-hypre=$HYPRE_BASE --with-silo=$SILO_BASE --with-amps-sequential-io_ make -j 14 make install Run tests cd ../test make check","title":"ParFlow on UAHPC"},{"location":"how_to_notes/HPC_systems/UA_HPC/#example-pbs-script","text":"This is an example of what a job submit script should look like. You can generate one based off of this or using the link above # Your job will use 1 node, 8 cores, and 48gb of memory total. #PBS -q standard #PBS -l select=1:ncpus=16:mem=48gb:pcmem=6gb ### Specify a name for the job #PBS -N test ### Specify the group name #PBS -W group_list=lecondon ### Walltime is how long your job will run #PBS -l walltime=00:50:00 ### Joins standard error and standard out #PBS -j oe cd /home/u18/lecondon/Test/washita/tcl_scripts tclsh Dist_Forcings.tcl tclsh LW_Test.tcl Note: the \u201cplace=pack:shared\u201d and \u201ccput\u201d lines in the HPC job builder guide website are no longer necessary as of July 2019.","title":"Example pbs script"},{"location":"how_to_notes/HPC_systems/UA_HPC/#slurm-scripts","text":"Puma uses SLURM to submit jobs instead of PBS. The link above has a PBS to SLURM Rosetta stone, so you can convert your PBS script. Otherwise, the Puma Quick start guide contains a sample SLURM script ( https://public.confluence.arizona.edu/display/UAHPC/Puma+Quick+Start#PumaQuickStart-WritingaSLURMSubmissionScript) ).","title":"SLURM scripts"},{"location":"how_to_notes/HPC_systems/UA_HPC/#debug-queue","text":"The debug queue is a high-priority queue with a short runtime that is used to debug your runs before sending them to the standard or other queue. More information can be found here: https://docs.hpc.arizona.edu/display/UAHPC/Allocation+and+Limits Example pbs script for debug queue (comments are in {} and not to be included in script): # Your job will use 1 node, 8 cores, and 48gb of memory total. #PBS -q debug {changing specified queue to the debug queue} #PBS -l select=1:ncpus=16:mem=48gb:pcmem=6gb {cannot exceed 2 nodes or 56 total cores} ### Specify a name for the job #PBS -N test ### Specify the group name #PBS -W group_list=lecondon ### Walltime is how long your job will run #PBS -l walltime=00:10:00 {walltime cannot exceed 10 minutes} ### Joins standard error and standard out #PBS -j oe Once the job has been submitted to the debug queue and been executed, the error and output messages for the job will indicate whether the job ran for the total allotted time or ran into an error before then.","title":"Debug queue"},{"location":"how_to_notes/HPC_systems/UA_HPC/#time-allocation","text":"Each PI has a finite amount of time allocation each month. After each run completes, the _run_name.o####### _file will show the amount of time left in the standard queue each month. Each time you submit a script, the system makes sure that your request can be fulfilled in the given queue - otherwise, it returns an error message. If you\u2019re requesting more time than you have for the rest of the month, you can: 1) reduce the amount of time you are requesting, 2) submit it to the windfall queue, or 3) wait until the end of the month and try again next month. Your advisor may not be okay with you trying option 3.","title":"Time Allocation"},{"location":"how_to_notes/HPC_systems/UA_HPC/#accessing-and-using-software-like-python","text":"General: https://public.confluence.arizona.edu/display/UAHPC/Accessing+Software Python: https://public.confluence.arizona.edu/display/UAHPC/Using+and+Installing+Python Conda: https://public.confluence.arizona.edu/display/UAHPC/Open+On+Demand Summary: Many software are installed on the server (> 100). Note that both Python and Anaconda exist, which allow for the installation of Python packages and usage of conda virtual environments.","title":"Accessing and using software (like Python)"},{"location":"how_to_notes/HPC_systems/UA_HPC/#python","text":"There are four versions of Python on Ocelote, (up to Python 3.6.5). Python 3.6.5 is loaded with \u2018module load python\u2019, and includes machine learning packages like Tensorflow. (Does it include PyTorch?) There are three versions of Python in ElGato (up to Python 3.8.0). Python 3.8.0 is loaded with \u2018module load python/3.8/3.8.0\u2019 There are two versions of Python in Puma (up to Python 3.8.2) Note that for DL software, Puma is best https://public.confluence.arizona.edu/display/UAHPC/Puma+Quick+Start","title":"Python:"},{"location":"how_to_notes/HPC_systems/UA_HPC/#virtual-environments","text":"It is useful to set up a virtualenv within your working directory.","title":"Virtual Environments"},{"location":"how_to_notes/HPC_systems/UA_HPC/#conda","text":"Anaconda is available as a module, with its requisite packages and access to virtual environments (too?) Look at Conda Environments: conda env list Create Conda Environments: conda create --name -myenv Activate Conda Environments: conda activate and conda deactivate","title":"Conda"},{"location":"how_to_notes/HPC_systems/UA_HPC/#how-to-do-a-basic-script-in-python","text":"Log in via terminal or bash see the \"Logging In\" section Find Python modules module avail python This will list availalbe python modules. e.g. python/3.6/3.65 Load Python module load python/3.6/3.65 Add/Create/Locate the script you want to run pytorch_helloworld.py -> print(\"hello world\") Create and edit pbs script (note important fields) touch pytorch_helloworld.pbs vim pytorch_helloworld.pbs # -------------------------------------------------------------- ### PART 1: Requests resources to run your job. # -------------------------------------------------------------- ### Optional. Set the job name #PBS -N pytorch_helloworld ### REQUIRED. Specify the PI group for this job #PBS -W group_list=lecondon ### Optional. Request email when job begins and ends ### PBS -m bea ### Optional. Specify email address to use for notification ### PBS -M roberthull@email.arizona.edu ### REQUIRED. Set the queue for your job. #PBS -q windfall ### REQUIRED. Set the number of nodes, cores and memory that will be used for this job ### pcmem is optional as it defaults to 6gb per core. Only required for high memory = 42gb. #PBS -l select=1:ncpus=1:mem=6gb:pcmem=6gb ### REQUIRED. Specify \"wallclock time\" required for this job, hhh:mm:ss #PBS -l walltime=0:1:0 ### Optional. cput = time x ncpus. Default: cput = walltime x ncpus. #PBS -l cput=0:1:0 # -------------------------------------------------------------- ### PART 2: Executes bash commands to run your job # -------------------------------------------------------------- ### Load required modules/libraries if needed module load python/3.6/3.6.5 ### change to your scripts directory cd ~/python ### run your work python3 pytorch_helloworld.py sleep 10i Submit job via qsub qsub pytorch_helloworld.pbs3592178.head1.cm.cluster Follow job progression via qstat View results of job <jobname>.o<number> for results, ` .e for errors Look at results: vim pytorch_helloworld.o3692196 -> hello world If there are errors use: vim pytorch_helloworld.e3692196","title":"How to do a basic script in Python"},{"location":"how_to_notes/HPC_systems/UA_HPC/#how-to-run-a-more-complicated-python-script-in-terminal-on-uahpc","text":"Log in via terminal or bash (go to Ocelote) See the \"Logging in\" section Log in via terminal or bash see the \"Logging In\" section Find Python modules module avail python This will list availalbe python modules. e.g. python/3.6/3.65 Load Python module load python/3.6/3.65 Take care that you load the version of python that you need. That includes considerations of what packages are available via UAHPC, and how you\u2019ve installed custom packages in your virtual environment Also, you can check what python versions are available and/or are loaded using module list Activate your virtual environment (create it if necessary) virtualenv --system-site-packages ~/mypyenv_oc_py.3.6 source ~/mypyenv_oc_py3.6/bin/activate Take care to use a virtual environment compatible with your version of python and with your super computer of choice (Ocelote v ElGato) Add/Create/Locate the script you want to run Pytorch_gpus.py \u2192 runs a simple gpu script Note if you use custom packages you will need to manually reference the location of your virtual environment in the following way within your python script: Check which packages you have in your virtual environment, pip list (note that python/3.6/3.6.5 for ocelote has almost everything you would ever want to have\u2026) Install any packages that you think you need to run your desired script pip install <name_of_package> Create and edit pbs script (note important fields) #!/bin/bash # -------------------------------------------------------------- ### PART 1: Requests resources to run your job. # -------------------------------------------------------------- ### Optinal. Set the job name #PBS -N pytorch_gpus_n1 ### REQUIRED. Specify the PI group for this job #PBS -W group_list=lecondon ### Optional. Request email when job begins and ends ### PBS -m bea ### Optional. Specify email address to use for notification ### PBS -M &lt;YOUR NETID>@email.arizona.edu ### REQUIRED. Set the queue for your job. #PBS -q windfall ### REQUIRED. Set the number of nodes, cores and memory that will be used for this job ### pcmem is optional as it defaults to 6gb per core. Only required for high memory = 42gb. #PBS -l select=1:ncpus=1:mem=6gb ### REQUIRED. Specify \"wallclock time\" required for this job, hhh:mm:ss #PBS -l walltime=0:1:0 ### Optional. cput = time x ncpus. Default: cput = walltime x ncpus. #PBS -l cput=0:1:0 # -------------------------------------------------------------- ### PART 2: Executes bash commands to run your job # -------------------------------------------------------------- ### Load required modules/libraries if needed module load python/3.6/3.6.5 #### Activate virtual environment source ~/mypyenv_oc_py3.6/bin/activate ### change to your script\u2019s directory cd ~/python ### Run your work python3 pytorch_gpus.py sleep 10 A quick note about Requesting Resources and Node Types (Standard, GPU, or High Memory) via the select statement. The basic `select statement is: #PBS -l select=x:ncpus=Y:mem=Zgb Where: x = The number of nodes or units of the resources required Y = The number of cores (individual processors) required on each node Z = The amount of memory (in mb or gb) required on each node \u2018Normal\u2019 Requests Ocelote For Ocelote all of the standard nodes have 28 cores and 6GB per core, pcmem=6gb can be added to the line or left off, and it will default to 6gm. The following select statement would request one complete node: #PBS -l select=28:ncpus=28:pcmem=6gb This is an example of a job that uses two nodes #PBS -l select=2:ncpus=28:mem=16gb El Gato On El Gato all of the standard nodes of 16 cores and 4GB per core. The maximum available memory on a standard El Gato node is 62GB, leaving the differences for the operating system. The following select statement would request one complete node #PBS -l select=1:ncpus=16:pcmem=4gb \u2018GPU\u2019 Requests Ocelote On Ocelote there are 46 Nvidia P100 nodes available with 28 cores and 224 GB of memory. Users of these nodes can use either their standard allocation of cpu hours or windfall time. Jobs that do not need GPU's will not run on them, including windfall jobs taht do no need the GPU. EAch job will have exclusive access to the node to prevent contention between the CPU cores and the GPU. Each group is limited to ten GPU nodes concurrently. This requests a CentOs 7 GPU node: #PBS -l select=1:ncpus=28:mem=224gb:np100s=1:os7=True or #PBS -l select=1:ncpus=28:mem=224gb:ngpus=1:os7=True This requests a CentOS 6 GPU node: #PBS -l select=1:ncpus=28:mem=224gb:np100s=1 Ocelote has a single node with two GPUs that may be requested with #PBS -l select=1:ncpus=28:mem=224gb:np100s=2 El Gato Unlike Ocelote, El Gato has cgroups enabled which allows for selecting a partial GPU node. Memory requests should be scaled by ncpusx16gb. To request a full GPU node: #PBS -l select=1:ncpus=16:mem=250gb:ngpus=1:pcmem=16gb To request a single node with two GPUs: #PBS -l select=1:ncpus=16:mem=250gb:ngpus=2:pcmem=16gb An example of a GPU pbs using pytorch is as follows #!/bin/bash # -------------------------------------------------------------- ### PART 1: Requests resources to run your job. # -------------------------------------------------------------- #PBS -q standard #PBS -l select=1:ncpus=28:mem=168gb:pcmem=6gb:np100s=1:os7=True ### Specify a name for the job #PBS -N pytorch_gpus_single3 ### Specify the group name #PBS -W group_list=lecondon ### Walltime is how long your job will run #PBS -l walltime=00:05:00 # -------------------------------------------------------------- ### PART 2: Executes bash commands to run your job # -------------------------------------------------------------- ### Load required modules/libraries if needed module load python/3.6/3.6.5 ### In case needed for gpu..? module load pytorch/nvidia/20.01 ### Activate virtual environment source ~/mypyenv_oc_py3.6/bin/activate ### change to your script\u2019s directory cd ~/python ### Run your work singularity exec --nv /cm/shared/uaapps/pytorch/20.01/nvidia-pytorch.20.01-py3.simg python pytorch_gpus.py sleep 10 Submit job via qsub qsub pytorch_gpus_n1.pbs2708168.head1.cm.cluster Follow job progression via qstat qstat pbs2708168 View results of job <jobname>.o<number> for results, <jobname>.e<number> for errors","title":"How to run a more complicated Python script in terminal on UAHPC"},{"location":"how_to_notes/HPC_systems/UA_HPC/#how-to-run-python-in-a-jupiter-notebook-on-uahpc","text":"It might be easiest to run Python using a Jupyter Notebook, for which UAHPC has developed super convenient GUIs. Check here for a great tutorial.","title":"How to run Python in a Jupiter notebook on UAHPC"},{"location":"how_to_notes/HPC_systems/cheyenne/","text":"Running on Cheyenne (NCAR) **Amanda Triplett ** Original: 12/2019 Update: 08/2020 Description ** **This gives an overview with examples of how to remotely access HPC resources at NCAR (specifically Cheyenne), update your bash profile and modules, an example job_script with the commands Cheyenne needs to run, how to run and check your job as well as useful information to transfer and edit your files. Software No specific software is needed, you do need an account with NCAR and an HPC time allotment in order to access resources. Everything can be done through your terminal. Accessing and logging in to Cheyenne Open your terminal and type the following: 1. ssh -Y -l \u201cyour_username\u201d cheyenne.ucar.edu 1. _Note:_ Macs often need the -Y and -l commands 2. If it asks if you want to continue connecting, say yes 2. Enter your password 3. You probably already set up duo when you set up your account, look on NCARs website for Cheyenne or call the help desk 3. You\u2019re logged in! Go to your scratch folder 4. Every user starts with a scratch folder under their username, once you navigate here, you can create file directories just as you do on your own computer 4. cd /glade/scratch/\u201dyour_username\u201d 5. Mkdir \u201cnew_directory_name\u201d Updating your bash profile and loading modules Make any necessary updates to your bash profile, directions below are requiredif you want to run ParFlow - you should only need to do this once and make sure you go to your scratch folder first vim ~/.bash_profile export PARFLOW_DIR=/glade/p/univ/ucsm0002/parflow3.7_0826_2020/parflow export HYPRE_DIR=/glade/p/univ/ucsm0002/hypre/2.10.1 export SILO_DIR=/glade/p/univ/ucsm0002/silo/4.10.2 source ~/.bash_profile Make sure correct modules are loaded Note: This may not always be necessary to do manually, but if you get errors such as mpi not loading correctly, load all the modules and see if that solves the problem. You do have to re-load the modules each time. Type command: module list a. You\u2019ll probably see something like below if you haven\u2019t made any changes: b. Currently Loaded Modules: 1. 1) ncarenv/1.3 2) intel/18.0.5 3) ncarcompilers/0.5.0 4) mpt/2.19 5) netcdf/4.6.3 Load the modules you need (example. below) c. aktriplett@cheyenne4:/glade/scratch/aktriplett> module load cmake 8. aktriplett@cheyenne4:/glade/scratch/aktriplett> module load nco e. aktriplett@cheyenne4:/glade/scratch/aktriplett> module load ncl f. aktriplett@cheyenne4:/glade/scratch/aktriplett> module load ncview g. aktriplett@cheyenne4:/glade/scratch/aktriplett> module load R h. aktriplett@cheyenne4:/glade/scratch/aktriplett> module list After you load the modules your list will look like this: Currently Loaded Modules: 1) ncarenv/1.3 3) ncarcompilers/0.5.0 5) netcdf/4.6.3 7) nco/4.7.9 9) ncview/2.1.7 2) intel/18.0.5 4) mpt/2.19 6) cmake/3.14.4 8) ncl/6.6.2 10) R/3.6.0 Getting files to and from Cheyenne using scp NOTE: This is an inferior method to using Globus , see the getting started on Globus notes in the Methods section. Getting files to Cheyenne To copy from your computer to Cheyenne: Navigate to the folder where your file is scp \u201cyour_file_name\u201d \u201cyour_user_name\u201d@data-access.ucar.edu:/glade/scratch/\u201dyour_user_name\u201d/\u201dthe rest of the path to where you want the file\u201d To copy from Cheyenne to your computer Navigate to the folder where you want the files scp \u201cyour_user_name\u201d@data-access.ucar.edu:/glade/scratch/\u201dyour_user_name\u201d/\u201dthe rest of the path to where you want the file\u201d . You can use a *.{\u201cfile_ext\u201d} to scp multiple files of the same type scp \u201cyour_user_name\u201d@data-access.ucar.edu:/glade/scratch/\u201dyour_user_name\u201d/\u201dthe rest of the path to where you want the file\u201d/*.{file_ext1, file_ext2} . Moving files within Cheyenne Navigate to where the file is now 14. mv \u201cfile_name\u201d \u201cdestination_file_path\u201d Editing and Saving Files within Cheyenne To open a file vi \u201cfile_name\u201d To make a change, type i for insert Click escape to stop making changes Type zz to save Type shift zz to save and exit Type :x to exit without saving Job Scripts, running your TCL, checking your status and killing your job You need a job_script for Cheyenne to run your job. There will be a tclsh call at the end of this that will call your tcl script and run your job. Below is an example job script #!/bin/bash #PBS -N HRB_PLtest1 -> this is what your run will be called #PBS -A UCSM0009 -> your award number #PBS -l walltime=01:00:00 -> clock time before your script is killed #PBS -q regular -> specify your queue, r is regular but there is also debug #PBS -j oe #PBS -o log1.oe -> name of the run log file you get #PBS -m abe #PBS -M [aktriplett@email.arizona.edu](mailto:aktriplett@email.arizona.edu) -> add this line to get run e-mail updates #PBS -l select=12:ncpus=36:mpiprocs=36 -> the processors or computing resources you are requesting Run the executable tclsh HRB_parkinglot.tcl -> the name of your tcl script Useful link with info about anatomy and making of pbs scripts: https://www.scribd.com/document/359446973/PBS-Queue-Commands Commands for run scripts qsub \u201cscript_name\u201d This runs your job qstat -u \u201cyour_username\u201d Check on the status of your job Add alias qjobs=\"qstat -u $USER\" to your bash profile to check your job status using \u2018qjobs\u2019 c. Qdel \u201cjob_number\u201d i. Kill your job Checking your Core Hour Usage Follow this link to the Systems Accounting Manager (SAM) system Sam.ucar.edu You can also look at this link for additional information about what is available on SAM https://www2.cisl.ucar.edu/user-support/systems-accounting-manager Log-in to the SAM system using your regular log-in info, you will have to authenticate with Duo push as well One logged in, you can go to the reports tab to track your usage activity, it will look something like this: Misc. info Information about where to store / what directories to make can be put in the tcl script, it will make whatever folder / file tree you specify in your scratch directory The amount of storage available on the scratch directory shouldn\u2019t be an issue for most runs To transfer files to and from the scratch directory, you can just scp from the scratch folder to home computer and back again, there is no special node to do this on or way like the UAHPC You can check log outputs to see if there are any errors Installing icommands on Cheyenne It is possible to install icommands on Cheyenne without any additional privilege in your own directory. _Icommands if NOT natively installed on Cheyenne unlike UAHPC. _Detailed can be found here, https://wiki.cyverse.org/wiki/display/DS/Setting+Up+iCommands#SettingUpiCommands-co Download the installer to your local computer (links can be found in https://wiki.cyverse.org/wiki/display/DS/Setting+Up+iCommands#SettingUpiCommands-co ) and upload to your directory in Cheyenne. Download the ubuntu 14 version for unprivileged users In your Cheyenne directory with the installer, type or copy command: sh irods-icommands-4.1.10-ubuntu-14.installer Then you will see: Where would you like to install it? [/home/cyverse-user] Expanding contents under /home/cyverse-user. Updating .bashrc done! To make the changes take effect in the current shell, you will need to source your .bashrc file ( vim ~/.bash_profile ). The following lines should be added: # iRODS iCommands support export IRODS_PLUGINS_HOME=/glade/u/home/yourusername/icommands/plugins/ export PATH=/glade/u/home/yourusername/icommands:$PATH Open a new terminal window, if necessary, and run ienv to check that the new version was installed. initialise your account: _iinit_ [data.cyverse.org](http://data.cyverse.org/) Port: 1247 User: yourusername Zone: iplant Then enter your cyverse password Then you can start use icommands. Running with Python on Cheyenne General instructions of how to get running with python can be found at: https://www2.cisl.ucar.edu/resources/python-\u2013-ncar-package-library In order to install your own packages, you will have to create your own virtual environment after following the steps in the above page and then activate as follows: NOTE: You do not have permission to write in the ncar virtual env, so to install packages not included there, you have to create your own ncar_pylib -c 20201220 /glade/work/$USER/my_npl_clone ncar_pylib my_npl_clone pip --no-cache-dir install PACKAGE_NAME If your virtualenv stops working, you may have to delete it and create a new one, just follow the above steps again and you should be good Building a New Version of ParFlow on Cheyenne (08/2020) from MM at CSM Adding specific ParFlow Builds to your bash profile. This is an example for a specific version, you need to add the path to the version you want to use after export: vim ~/.bash_profile . export PARFLOW_DIR=/glade/p/univ/ucsm0002/parflow_build/parflow #(this is the folder where you will put the new version of Parflow) export HYPRE_DIR=/glade/p/univ/ucsm0002/hypre/2.10.1 export SILO_DIR=/glade/p/univ/ucsm0002/silo/4.10.2 Build instructions Please load the following modules: module load impi module load intel/17.0.1 module load cmake To see which modules are available, use command module avail . To check which modules you have loaded, you can use module list . For me, this returns: Currently Loaded Modules: 1) ncarenv/1.3 3) ncarcompilers/0.5.0 5) cmake/3.14.4 7) nco/4.7.9 9) netcdf/4.6.3 2) intel/17.0.1 4) impi/2017.1.132 6) ncl/6.6.2 (H) 8) ncview/2.1.7 10) R/3.6.0 I was able to follow the typical installing directions for Silo and Hypre (from the parflow blog) and set them in my bash profile. Then just follow the cmake workflow for installing parflow: cd ~/glade/p/univ/ucsm0002/parflow_build (or wherever) git clone -b master http://github.com/parflow/parflow.git export PARFLOW_DIR=~//glade/p/univ/ucsm0002/parflow_build/parflow (or wherever. Add to bash_profile.) cd $PARFLOW_DIR mkdir build cd build cmake .. -DCMAKE_INSTALL_PREFIX=$PARFLOW_DIR -DPARFLOW_AMPS_LAYER=mpi1 -DPARFLOW_ENABLE_TIMING=TRUE -DHYPRE_ROOT=$HYPRE_DIR -DSILO_ROOT=$SILO_DIR -DPARFLOW_HAVE_CLM=ON -DPARFLOW_AMPS_SEQUENTIAL_IO=TRUE make make install Go to where you installed","title":"Running on Cheyenne (NCAR)"},{"location":"how_to_notes/HPC_systems/cheyenne/#running-on-cheyenne-ncar","text":"**Amanda Triplett ** Original: 12/2019 Update: 08/2020","title":"Running on Cheyenne (NCAR)"},{"location":"how_to_notes/HPC_systems/cheyenne/#description","text":"** **This gives an overview with examples of how to remotely access HPC resources at NCAR (specifically Cheyenne), update your bash profile and modules, an example job_script with the commands Cheyenne needs to run, how to run and check your job as well as useful information to transfer and edit your files.","title":"Description"},{"location":"how_to_notes/HPC_systems/cheyenne/#software","text":"No specific software is needed, you do need an account with NCAR and an HPC time allotment in order to access resources. Everything can be done through your terminal.","title":"Software"},{"location":"how_to_notes/HPC_systems/cheyenne/#accessing-and-logging-in-to-cheyenne","text":"Open your terminal and type the following: 1. ssh -Y -l \u201cyour_username\u201d cheyenne.ucar.edu 1. _Note:_ Macs often need the -Y and -l commands 2. If it asks if you want to continue connecting, say yes 2. Enter your password 3. You probably already set up duo when you set up your account, look on NCARs website for Cheyenne or call the help desk 3. You\u2019re logged in! Go to your scratch folder 4. Every user starts with a scratch folder under their username, once you navigate here, you can create file directories just as you do on your own computer 4. cd /glade/scratch/\u201dyour_username\u201d 5. Mkdir \u201cnew_directory_name\u201d","title":"Accessing and logging in to Cheyenne"},{"location":"how_to_notes/HPC_systems/cheyenne/#updating-your-bash-profile-and-loading-modules","text":"Make any necessary updates to your bash profile, directions below are requiredif you want to run ParFlow - you should only need to do this once and make sure you go to your scratch folder first vim ~/.bash_profile export PARFLOW_DIR=/glade/p/univ/ucsm0002/parflow3.7_0826_2020/parflow export HYPRE_DIR=/glade/p/univ/ucsm0002/hypre/2.10.1 export SILO_DIR=/glade/p/univ/ucsm0002/silo/4.10.2 source ~/.bash_profile Make sure correct modules are loaded Note: This may not always be necessary to do manually, but if you get errors such as mpi not loading correctly, load all the modules and see if that solves the problem. You do have to re-load the modules each time. Type command: module list a. You\u2019ll probably see something like below if you haven\u2019t made any changes: b. Currently Loaded Modules: 1. 1) ncarenv/1.3 2) intel/18.0.5 3) ncarcompilers/0.5.0 4) mpt/2.19 5) netcdf/4.6.3 Load the modules you need (example. below) c. aktriplett@cheyenne4:/glade/scratch/aktriplett> module load cmake 8. aktriplett@cheyenne4:/glade/scratch/aktriplett> module load nco e. aktriplett@cheyenne4:/glade/scratch/aktriplett> module load ncl f. aktriplett@cheyenne4:/glade/scratch/aktriplett> module load ncview g. aktriplett@cheyenne4:/glade/scratch/aktriplett> module load R h. aktriplett@cheyenne4:/glade/scratch/aktriplett> module list After you load the modules your list will look like this: Currently Loaded Modules: 1) ncarenv/1.3 3) ncarcompilers/0.5.0 5) netcdf/4.6.3 7) nco/4.7.9 9) ncview/2.1.7 2) intel/18.0.5 4) mpt/2.19 6) cmake/3.14.4 8) ncl/6.6.2 10) R/3.6.0","title":"Updating your bash profile and loading modules"},{"location":"how_to_notes/HPC_systems/cheyenne/#getting-files-to-and-from-cheyenne-using-scp","text":"NOTE: This is an inferior method to using Globus , see the getting started on Globus notes in the Methods section. Getting files to Cheyenne To copy from your computer to Cheyenne: Navigate to the folder where your file is scp \u201cyour_file_name\u201d \u201cyour_user_name\u201d@data-access.ucar.edu:/glade/scratch/\u201dyour_user_name\u201d/\u201dthe rest of the path to where you want the file\u201d To copy from Cheyenne to your computer Navigate to the folder where you want the files scp \u201cyour_user_name\u201d@data-access.ucar.edu:/glade/scratch/\u201dyour_user_name\u201d/\u201dthe rest of the path to where you want the file\u201d . You can use a *.{\u201cfile_ext\u201d} to scp multiple files of the same type scp \u201cyour_user_name\u201d@data-access.ucar.edu:/glade/scratch/\u201dyour_user_name\u201d/\u201dthe rest of the path to where you want the file\u201d/*.{file_ext1, file_ext2} . Moving files within Cheyenne Navigate to where the file is now 14. mv \u201cfile_name\u201d \u201cdestination_file_path\u201d","title":"Getting files to and from Cheyenne using scp"},{"location":"how_to_notes/HPC_systems/cheyenne/#editing-and-saving-files-within-cheyenne","text":"To open a file vi \u201cfile_name\u201d To make a change, type i for insert Click escape to stop making changes Type zz to save Type shift zz to save and exit Type :x to exit without saving","title":"Editing and Saving Files within Cheyenne"},{"location":"how_to_notes/HPC_systems/cheyenne/#job-scripts-running-your-tcl-checking-your-status-and-killing-your-job","text":"You need a job_script for Cheyenne to run your job. There will be a tclsh call at the end of this that will call your tcl script and run your job. Below is an example job script #!/bin/bash #PBS -N HRB_PLtest1 -> this is what your run will be called #PBS -A UCSM0009 -> your award number #PBS -l walltime=01:00:00 -> clock time before your script is killed #PBS -q regular -> specify your queue, r is regular but there is also debug #PBS -j oe #PBS -o log1.oe -> name of the run log file you get #PBS -m abe #PBS -M [aktriplett@email.arizona.edu](mailto:aktriplett@email.arizona.edu) -> add this line to get run e-mail updates #PBS -l select=12:ncpus=36:mpiprocs=36 -> the processors or computing resources you are requesting","title":"Job Scripts, running your TCL, checking your status and killing your job"},{"location":"how_to_notes/HPC_systems/cheyenne/#run-the-executable","text":"tclsh HRB_parkinglot.tcl -> the name of your tcl script Useful link with info about anatomy and making of pbs scripts: https://www.scribd.com/document/359446973/PBS-Queue-Commands Commands for run scripts qsub \u201cscript_name\u201d This runs your job qstat -u \u201cyour_username\u201d Check on the status of your job Add alias qjobs=\"qstat -u $USER\" to your bash profile to check your job status using \u2018qjobs\u2019 c. Qdel \u201cjob_number\u201d i. Kill your job","title":"Run the executable"},{"location":"how_to_notes/HPC_systems/cheyenne/#checking-your-core-hour-usage","text":"Follow this link to the Systems Accounting Manager (SAM) system Sam.ucar.edu You can also look at this link for additional information about what is available on SAM https://www2.cisl.ucar.edu/user-support/systems-accounting-manager Log-in to the SAM system using your regular log-in info, you will have to authenticate with Duo push as well One logged in, you can go to the reports tab to track your usage activity, it will look something like this:","title":"Checking your Core Hour Usage"},{"location":"how_to_notes/HPC_systems/cheyenne/#misc-info","text":"Information about where to store / what directories to make can be put in the tcl script, it will make whatever folder / file tree you specify in your scratch directory The amount of storage available on the scratch directory shouldn\u2019t be an issue for most runs To transfer files to and from the scratch directory, you can just scp from the scratch folder to home computer and back again, there is no special node to do this on or way like the UAHPC You can check log outputs to see if there are any errors","title":"Misc. info"},{"location":"how_to_notes/HPC_systems/cheyenne/#installing-icommands-on-cheyenne","text":"It is possible to install icommands on Cheyenne without any additional privilege in your own directory. _Icommands if NOT natively installed on Cheyenne unlike UAHPC. _Detailed can be found here, https://wiki.cyverse.org/wiki/display/DS/Setting+Up+iCommands#SettingUpiCommands-co Download the installer to your local computer (links can be found in https://wiki.cyverse.org/wiki/display/DS/Setting+Up+iCommands#SettingUpiCommands-co ) and upload to your directory in Cheyenne. Download the ubuntu 14 version for unprivileged users In your Cheyenne directory with the installer, type or copy command: sh irods-icommands-4.1.10-ubuntu-14.installer Then you will see: Where would you like to install it? [/home/cyverse-user] Expanding contents under /home/cyverse-user. Updating .bashrc done! To make the changes take effect in the current shell, you will need to source your .bashrc file ( vim ~/.bash_profile ). The following lines should be added: # iRODS iCommands support export IRODS_PLUGINS_HOME=/glade/u/home/yourusername/icommands/plugins/ export PATH=/glade/u/home/yourusername/icommands:$PATH Open a new terminal window, if necessary, and run ienv to check that the new version was installed. initialise your account: _iinit_ [data.cyverse.org](http://data.cyverse.org/) Port: 1247 User: yourusername Zone: iplant Then enter your cyverse password Then you can start use icommands.","title":"Installing icommands on Cheyenne"},{"location":"how_to_notes/HPC_systems/cheyenne/#running-with-python-on-cheyenne","text":"General instructions of how to get running with python can be found at: https://www2.cisl.ucar.edu/resources/python-\u2013-ncar-package-library In order to install your own packages, you will have to create your own virtual environment after following the steps in the above page and then activate as follows: NOTE: You do not have permission to write in the ncar virtual env, so to install packages not included there, you have to create your own ncar_pylib -c 20201220 /glade/work/$USER/my_npl_clone ncar_pylib my_npl_clone pip --no-cache-dir install PACKAGE_NAME If your virtualenv stops working, you may have to delete it and create a new one, just follow the above steps again and you should be good","title":"Running with Python on Cheyenne"},{"location":"how_to_notes/HPC_systems/cheyenne/#building-a-new-version-of-parflow-on-cheyenne-082020-from-mm-at-csm","text":"Adding specific ParFlow Builds to your bash profile. This is an example for a specific version, you need to add the path to the version you want to use after export: vim ~/.bash_profile . export PARFLOW_DIR=/glade/p/univ/ucsm0002/parflow_build/parflow #(this is the folder where you will put the new version of Parflow) export HYPRE_DIR=/glade/p/univ/ucsm0002/hypre/2.10.1 export SILO_DIR=/glade/p/univ/ucsm0002/silo/4.10.2 Build instructions Please load the following modules: module load impi module load intel/17.0.1 module load cmake To see which modules are available, use command module avail . To check which modules you have loaded, you can use module list . For me, this returns: Currently Loaded Modules: 1) ncarenv/1.3 3) ncarcompilers/0.5.0 5) cmake/3.14.4 7) nco/4.7.9 9) netcdf/4.6.3 2) intel/17.0.1 4) impi/2017.1.132 6) ncl/6.6.2 (H) 8) ncview/2.1.7 10) R/3.6.0 I was able to follow the typical installing directions for Silo and Hypre (from the parflow blog) and set them in my bash profile. Then just follow the cmake workflow for installing parflow: cd ~/glade/p/univ/ucsm0002/parflow_build (or wherever) git clone -b master http://github.com/parflow/parflow.git export PARFLOW_DIR=~//glade/p/univ/ucsm0002/parflow_build/parflow (or wherever. Add to bash_profile.) cd $PARFLOW_DIR mkdir build cd build cmake .. -DCMAKE_INSTALL_PREFIX=$PARFLOW_DIR -DPARFLOW_AMPS_LAYER=mpi1 -DPARFLOW_ENABLE_TIMING=TRUE -DHYPRE_ROOT=$HYPRE_DIR -DSILO_ROOT=$SILO_DIR -DPARFLOW_HAVE_CLM=ON -DPARFLOW_AMPS_SEQUENTIAL_IO=TRUE make make install Go to where you installed","title":"Building a New Version of ParFlow on Cheyenne (08/2020) from MM at CSM"},{"location":"how_to_notes/HPC_systems/cori/","text":"Cori Jun Zhang, 04/24/2019 Description Running notes with links and tips for running ParFlow on Cori. Feel free to update and add to this document as needed. Useful links Getting started: http://www.nersc.gov/users/computational-systems/cori/ Slides for a short training course are available at Reference Docs/Cori_Training_course obtaining an account ** Sep/2021 ** Apply an account according to the instructions here . Obtain the project number before applying the account. ** Mar/2019 ** Ask Laura to create a username for you at the NERSC, you will get an email to submit an application. Once they approve it, a second email will be sent to active the account (have to be activated within 72 hours). Logging in Before you can login any host, you have to create a MFA token for your accounts, instructions are available in https://docs.nersc.gov/connect/mfa/ Log onto cori with you NERSC username, password and OTP(one-time password) from your token. Our repo number is m2511 (this project is no longer activated). ssh username@cori.nersc.gov The CONUS project directory is (this project is no longer activated). /project/projectdirs/m2511 You should change your default shell to bash and you can do it by logging into nim.nersc.gov. In the \u2018Profile\u2019 tab, scroll down to \u2018Server Login\u2019, click the \u2018Edit\u2019 option of \u2018cori\u2019, choose \u2018/bin/bash\u2019 in the \u2018Login Shell\u2019. Transferring files Use scp to transfer files between your local machine and cori for small transfer. scp your/local/file username@cori.nersc.gov:your/cori/directory Globus the recommended tool for moving data in and out of NERSC for large transfers http://www.globus.org/ or http://globus.nersc.gov/ See also the group notes on Globus User scratch directory: /global/cscratch1/sd/yourusername Setting environments https://docs.nersc.gov/environment/ NOTE that you should not modify your bash_profile directly. Instead you make changes to .bash_profile.ext (mine is here: /global/homes/j/jzhang55). Also note that your home directory is the same across Cori and Hopper and you can put settings in this file that are specific to what machine you are running on. You should add PARFLOW_DIR in your .bash_profile.ext as following: export PARFLOW_DIR=/global/project/projectdirs/m2511/parflow/cori-v3.7.0-11-gf70ce58-2021-07-14 Running scripts https://docs.nersc.gov/jobs/ You cannot run on your own directory, have to submit a job to the queue. Check out the queue policies because they have different \u2018charge factors\u2019 for different queues: https://docs.nersc.gov/jobs/policy/ You need a job script to specify your job description, and use sbatch to submit the job. The script examples can be found https://docs.nersc.gov/jobs/examples/ and also at the end of this file. If you prepare your job script or input files in your local computer, it may result in error (Batch script contains DOS line breaks (\\r\\n). Use command dos2unix yourfilename. Cori also provides an online job script generator, which is easy to use and can be found in MyNERSC -> Jobs -> Jobscript generator https://my.nersc.gov/script_generator.php To run parflow Use tclsh on the command line before submitting the job to the queue. Job script example #!/bin/bash -l #SBATCH --qos=debug #SBATCH --nodes=2 #SBATCH --constraint=haswell #SBATCH --time=00:10:00 {the maximum running time} #SBATCH --job-name=test #SBATCH --account=m3780 export PARFLOW_DIR=/global/project/projectdirs/m2511/parflow/cori-v3.7.0-11-gf70ce58-2021-07-14 source $PARFLOW_DIR/setenv.sh {these two lines are required to run} cd /global/cscratch1/sd/jzhang55/test/ {cd your directory} srun -n 64 $PARFLOW_DIR/bin/parflow test {srun -n 64 means you request 64/32=2 nodes to run, so #SBATCH --nodes=2} Monitoring your job https://docs.nersc.gov/jobs/#submitting-jobs https://docs.nersc.gov/jobs/#monitoring-jobs sbatch : submit a batch script sacct : display accounting data for jobs and job steps salloc : request nodes for an interactive batch session srun : launch parallel jobs scancel : delete a batch job sqs : NERSC custom queue display with job priority ranking info squeue : display info about jobs in the queue sinfo : view SLURM configuration about nodes and partitions scontrol : view and modify SLURM configuration and job state","title":"Cori"},{"location":"how_to_notes/HPC_systems/cori/#cori","text":"Jun Zhang, 04/24/2019","title":"Cori"},{"location":"how_to_notes/HPC_systems/cori/#description","text":"Running notes with links and tips for running ParFlow on Cori. Feel free to update and add to this document as needed.","title":"Description"},{"location":"how_to_notes/HPC_systems/cori/#useful-links","text":"Getting started: http://www.nersc.gov/users/computational-systems/cori/ Slides for a short training course are available at Reference Docs/Cori_Training_course","title":"Useful links"},{"location":"how_to_notes/HPC_systems/cori/#obtaining-an-account","text":"** Sep/2021 ** Apply an account according to the instructions here . Obtain the project number before applying the account. ** Mar/2019 ** Ask Laura to create a username for you at the NERSC, you will get an email to submit an application. Once they approve it, a second email will be sent to active the account (have to be activated within 72 hours).","title":"obtaining an account"},{"location":"how_to_notes/HPC_systems/cori/#logging-in","text":"Before you can login any host, you have to create a MFA token for your accounts, instructions are available in https://docs.nersc.gov/connect/mfa/ Log onto cori with you NERSC username, password and OTP(one-time password) from your token. Our repo number is m2511 (this project is no longer activated). ssh username@cori.nersc.gov The CONUS project directory is (this project is no longer activated). /project/projectdirs/m2511 You should change your default shell to bash and you can do it by logging into nim.nersc.gov. In the \u2018Profile\u2019 tab, scroll down to \u2018Server Login\u2019, click the \u2018Edit\u2019 option of \u2018cori\u2019, choose \u2018/bin/bash\u2019 in the \u2018Login Shell\u2019. Transferring files Use scp to transfer files between your local machine and cori for small transfer. scp your/local/file username@cori.nersc.gov:your/cori/directory","title":"Logging in"},{"location":"how_to_notes/HPC_systems/cori/#globus","text":"the recommended tool for moving data in and out of NERSC for large transfers http://www.globus.org/ or http://globus.nersc.gov/ See also the group notes on Globus User scratch directory: /global/cscratch1/sd/yourusername","title":"Globus"},{"location":"how_to_notes/HPC_systems/cori/#setting-environments","text":"https://docs.nersc.gov/environment/ NOTE that you should not modify your bash_profile directly. Instead you make changes to .bash_profile.ext (mine is here: /global/homes/j/jzhang55). Also note that your home directory is the same across Cori and Hopper and you can put settings in this file that are specific to what machine you are running on. You should add PARFLOW_DIR in your .bash_profile.ext as following: export PARFLOW_DIR=/global/project/projectdirs/m2511/parflow/cori-v3.7.0-11-gf70ce58-2021-07-14 Running scripts https://docs.nersc.gov/jobs/ You cannot run on your own directory, have to submit a job to the queue. Check out the queue policies because they have different \u2018charge factors\u2019 for different queues: https://docs.nersc.gov/jobs/policy/ You need a job script to specify your job description, and use sbatch to submit the job. The script examples can be found https://docs.nersc.gov/jobs/examples/ and also at the end of this file. If you prepare your job script or input files in your local computer, it may result in error (Batch script contains DOS line breaks (\\r\\n). Use command dos2unix yourfilename. Cori also provides an online job script generator, which is easy to use and can be found in MyNERSC -> Jobs -> Jobscript generator https://my.nersc.gov/script_generator.php","title":"Setting environments"},{"location":"how_to_notes/HPC_systems/cori/#to-run-parflow","text":"Use tclsh on the command line before submitting the job to the queue. Job script example #!/bin/bash -l #SBATCH --qos=debug #SBATCH --nodes=2 #SBATCH --constraint=haswell #SBATCH --time=00:10:00 {the maximum running time} #SBATCH --job-name=test #SBATCH --account=m3780 export PARFLOW_DIR=/global/project/projectdirs/m2511/parflow/cori-v3.7.0-11-gf70ce58-2021-07-14 source $PARFLOW_DIR/setenv.sh {these two lines are required to run} cd /global/cscratch1/sd/jzhang55/test/ {cd your directory} srun -n 64 $PARFLOW_DIR/bin/parflow test {srun -n 64 means you request 64/32=2 nodes to run, so #SBATCH --nodes=2}","title":"To run parflow"},{"location":"how_to_notes/HPC_systems/cori/#monitoring-your-job","text":"https://docs.nersc.gov/jobs/#submitting-jobs https://docs.nersc.gov/jobs/#monitoring-jobs sbatch : submit a batch script sacct : display accounting data for jobs and job steps salloc : request nodes for an interactive batch session srun : launch parallel jobs scancel : delete a batch job sqs : NERSC custom queue display with job priority ranking info squeue : display info about jobs in the queue sinfo : view SLURM configuration about nodes and partitions scontrol : view and modify SLURM configuration and job state","title":"Monitoring your job"},{"location":"how_to_notes/HPC_systems/parflow_install/","text":"ParFlow Install Laura Condon, 2/13/19 Description A running set of notes on ParFlow installs for various platforms. Please update this with your experiences either by adding comments or inserting text. Please Add new installs to the top of this doc and use headings so that they can be included in the table of contents. **NOTE: If you are installing on a mac refer first to the ParFlow blog http://parflow.blogspot.com/ and look for the latest version in the compiling section which corresponds to your operating system. ** Links: You should also refer to the ParFlow blog for additional resources http://parflow.blogspot.com/ UA-HPC Ocelote Notes from Laura Fall 2018 Contacts Ric Anderson: ric@email.arizona.edu Chris Reidy: chrisreidy@email.arizona.edu Software install request form https://it.arizona.edu/service-request-forms They built Hypre and silo and these are available as modules Module commands https://docs.hpc.arizona.edu/display/UAHPC/Accessing+Software Module list (gives loaded modules) Module avail (gives list of all modules) Module show NAME (gives info on a specific module Job submit and queue information Ocelote uses pbs for its job management system. Example pbs script for running on 1 node: ### Your job will use 1 node, 28 cores, and 168gb of memory total. #PBS -q standard #PBS -l select=1:ncpus=28:mem=168gb:pcmem=6gb ### Specify a name for the job #PBS -N job_name ### Specify the group name #PBS -W group_list=lecondon ### Walltime is how long your job will run #PBS -l walltime=00:50:00 Example pbs script for debugging script (always a good idea before you run anything): ### Your job will use 1 node, 28 cores, and 168gb of memory total. #PBS -q debug #PBS -l select=1:ncpus=28:mem=168gb:pcmem=6gb ### Specify a name for the job #PBS -N job_name ### Specify the group name #PBS -W group_list=lecondon ### Walltime is how long your job will run #PBS -l walltime=00:05:00 qsub job_name.pbs (submits jobs) qstat (lists all jobs running and in queue) Building ParFlow ssh_ _username@hpc.arizona.edu Download ParFlow git clone https://github.com/parflow/parflow Rename this directory to whatever you want Setup environment vi ~/.bashrc Add the following lines to the user specified portion: module load gcc module load hypre module load silo export PARFLOW_DIR=/home/PATH_TO_YOUR_PARFLOW_DIR PATH=$PATH:$PARFLOW_DIR/bin source ~/.bashrc Build ParFlow cd pfsimulator ./configure --prefix=$PARFLOW_DIR --with-amps=mpi1 --with-clm --with-hypre=$HYPRE_BASE --with-silo=$SILO_BASE --with-amps-sequential-io make -j 14 make install Build PFtools cd ../pftools ./configure --prefix=$PARFLOW_DIR --with-amps=mpi1 --with-clm --with-hypre=$HYPRE_BASE --with-silo=$SILO_BASE --with-amps-sequential-io make -j 14 make install Run tests cd ../test make check MacOS Mojave These notes are from Nick Engdahl 2/12i/2019: This post will describe a \u201ctypical\u201d ParFlow installation, that is one that uses the most common options and configurations on macOS using the default bash shell. We\u2019ll primarily use the command line for this so go ahead and open a terminal window (if you don\u2019t know where terminal is use spotlight search). Preliminaries For commands entered into the terminal window I\u2019ll use the following convention where the command is preceded with \u201c >> \u201d for example: >> make {and here\u2019s an example comment/note you don\u2019t type} Anything that follows in curly brackets { } is just a note or comment and is NOT to be entered. OK, first thing\u2019s first. We need to define a few environment variables for the installation. Navigate to your home folder and create (or edit) your bash_profile: >> cd ~ >> vim .bash_profile In vim, hit \u201ci\u201d to enter insert mode and add the following lines, preferably at the bottom: export PARFLOW_DIR=~/ParF/parflow export HYPRE_DIR=~/ParF/hypre export SILO_DIR=~/ParF/silo export CC=clang export CXX=clang++ export FC=gfortran export F77=gfortran then press \u201cescape\u201d followed by \u201c:\u201d then \u201cx\u201d to save the changes and exit. To apply the changes: >> source .bash_profile This has now defined some environment variables and shortcuts we\u2019ll take advantage of later on. Next, we\u2019ll create a directory to hold all the various bits that comprise a typical installation. From your home directory, make a folder called \u201cParF\u201d and enter it: >> cd ~ {only necessary if you\u2019ve navigated out of home} >> mkdir ParF >> cd ParF There are 6 \u201cparts\u201d to these instructions and we\u2019re going to handle each separately: 1) install gcc/gfortran, 2) install cmake, 3) install OpenMPI, 4) Install Silo, 5) Install Hypre, 6) install ParFlow. Everything except step 6 are supporting libraries or programs that ParFlow needs for our typical installation. Step 1) Installing gfortran and gcc Truth be told we only really need gfortran but gcc is nice to have, and since they\u2019re often bundled it makes sense to knock both out. You could try building them from source code, but the easier way is to use a binary that is pre-built for MacOS. Those can be downloaded here: http://hpc.sourceforge.net The option you want is usually the first entry right below the word \u201cBinaries\u201d after the \u201cbig\u201d paragraph. Today, for me that is \u201cgcc-8.1-bin.tar.gz\u201d and clicking on that link put it in my \u201cDownloads\u201d folder. You don\u2019t need to bother with the \u201cgfortran\u201d link because it\u2019s already included in this one. In your terminal window, we first need to enable compiling programs on your mac. This is necessary after every major MacOS upgrade before you can compile new programs (old one will be fine though). >> sudo xcode-select --install A confirmation window will likely open and after you OK through all of that, we\u2019re done with that. You might also see a message saying command line tools are already installed, which is also fine. Next, navigate to wherever you put that download and extract it to /usr/local with: >> cd ~/Downloads {assuming your Download went here} >> sudo tar -xvf gcc-8.1-bin.tar -C / A lot of text will fly by then you\u2019ll get a command prompt. To make sure this has copied correctly, at the command prompt type: >> gfortran -v And you should get something like this: Using built-in specs. COLLECT_GCC=gfortran COLLECT_LTO_WRAPPER=/usr/local/libexec/gcc/x86_64-apple-darwin17.5.0/8.1.0/lto-wrapper Target: x86_64-apple-darwin17.5.0 Configured with: ../gcc-8.1.0/configure --enable-languages=c++,fortran Thread model: posix gcc version 8.1.0 (GCC) There is one last critically important step we need to take that was not necessary in the past. We need to install the package in MacOS that has some of the libraries we need in order to compile programs. These are located in \u201c/Library/Developer/CommandLineTools/Packages/\u201d and the easiest way to get there is to open a finder window, press command+shift+g, then paste in that path. There is only one package in this folder (macOS_SDK_headers_for_macOS_10.14.pkg), though its name might change, and to install it just double click on it and follow the instructions. I would also suggest you run \u201csudo --xcode-select --s /Library/Developer/CommandLineTools\u201d to make sure the system is pointing to the right version. Apple may be depreciating the package in future macOS releases, but we\u2019ll update you if/when that happens. Step 2) Installing cmake Past installation instructions have used the GNU autoconfig system. We\u2019ll still use that for some of the supporting components, but ParFlow itself is now built using the CMake interface. This isn\u2019t installed on your mac by default, so we\u2019ll start by adding it. We\u2019re going to download the latest version of the CMake source. There is also a GUI for CMake but we\u2019re not going to use it. Before proceeding, check here: https://cmake.org/download/ for the latest version. On the download page, go ahead and click on the link for the archive in the Unix/Linux source box and the download should complete shortly. Assuming the archive is in your ~/Downloads/ folder, we\u2019ll copy it into ParF, the expand the archive: As of this writing that file is \u201ccmake-3.14.0-rc1.tar.gz\u201d To move that download directly into the ParF folder, in your terminal window, be sure you\u2019re in the ParF directory, then type: >> cd ~/ParF/ >> cp ~/Downloads/cmake-3.14.0-rc1.tar . >> tar -xvf cmake-3.14.0-rc1.tar Note the period at the end of the second line. You will need to update the file name as newer versions of CMake are released. Now we\u2019ll enter the new directory and start building using cmake\u2019s helper: >> cd cmake-3.14.0-rc1 >> ./bootstrap {This will take a while} >> make {This will also take a while} >> sudo make install _ At the end of the make process you\u2019ll see percentage signs on the left edge of your screen and if it makes it to 100 without errors, you\u2019re all set. *From Jun: When installing cmake , on the step of ./bootstrap , sometimes it will pop up an error message that \u2018Unable to find any JVMs matching version\u2019, just follow the instruction to install the up-to-date JAVA\u2019 Step 3) Installing OpenMPI OpenMPI is the message passing interface we use for ParFlow (note that this is not the same as OpenMP). This is also what makes it parallel so we\u2019ll do this step next. Navigate back to your ParF directory: >> cd ~/ParF Next we\u2019re going to download openmpi. The simplest way to do that is from the command line using the \u201ccurl\u201d command, which is like \u201cwget\u201d for a mac. However, before doing so you might want to check to make sure you\u2019re downloading the most recent version at \u201chttps://www.open-mpi.org/software/ompi/\u201d You\u2019ll need to modify the version numbers if using a more recent release but otherwise it\u2019ll be just like this: >> curl \u201chttps://download.open-mpi.org/release/open-mpi/v4.0/openmpi-4.0.0.tar.gz\u201d -o \u201copenmpi-4.0.0.tar.gz\u201d (Note: When copying and pasting, sometimes quotes get messed up. If you get an error from that command about libcurl, just delete then re-add the quotation marks manually once you\u2019re in the terminal window.) Next, we\u2019ll decompress and extract the archive in one step: >> gunzip -c openmpi-4.0.0.tar.gz | tar xf - Next, we\u2019ll configure the installation package for our system. Navigate into the newly created openmpi-4.0.0 folder and type: >> ./configure --prefix=/usr/local CC=/usr/bin/clang CXX=/usr/bin/clang++ Those CC and CXX arguments force the computer to use a specific compiler, which is clang. On most systems this should install fine without those, but it doesn\u2019t hurt to have them. After you hit return, lots of stuff will fly by your screen for a long time. Once that\u2019s done, type: >> sudo make all install which will take forever and a decade before it completes, but then that\u2019s it; OpenMPI is now installed. If you want to check, type \u201cmpirun\u201d at the command line, but not much will happen since it can\u2019t find anything to do. Important: Remember that sudo is the \u201cnuclear option\u201d on unix-esque computers and should only be used when absolutely necessary; the install path for OpenMPI is inside a protected folder so we need to invoke sudo in this case, otherwise the installation can\u2019t go to the default. You could specify an alternate location, but since most applications that might use OpenMPI will look here, we might as well. Step 4) Installing Silo Silo is a file format that is used as the default for viewing ParFlow outputs, but we also now have other options available. Many of the test cases use it so we always recommend installing it. The latest version of silo can be found at: https://wci.llnl.gov/simulation/computer-codes/silo/downloads You could download the archive in a web browser and move it but instead I\u2019m going to use curl again. Navigate to the ParF folder and the commands are: >> cd ~/ParF >> curl \u201chttps://wci.llnl.gov/content/assets/docs/simulation/computer-codes/silo/silo-4.10.2/silo-4.10.2.tar.gz\u201d -o \u201csilo-4.10.2.tar.gz\u201d Mimicking the process we used for OpenMPI, to expand and extract the archive type: >> gunzip -c silo-4.10.2.tar.gz | tar xf - Now we\u2019ll rename the newly expanded directory to match our environment variable SILO_DIR: >> mv silo-4.10.2 silo Now we\u2019ll navigate into the new directory, configure, and install: >> cd silo >> ./configure --disable-silex >> make install >> cd .. And that completes the installation. Note that if your version of Silo differs, you\u2019ll need to update the appropriate file/directory names environment variable we set to reflect your installation path. Step 5) Installing the HYPRE library HYPRE is a library of numerical solvers for high-performance computing applications and ParFlow hooks into it for some of its functionality. As before the simplest way to get this is curl. There were some issues with hypre version 10 and ParFlow but the current release (2.11.2) or newer should work just fine. Installing this is nothing more than a minor variation of what we\u2019ve already done: >> cd ~/ParF >> curl \u201chttps://computating.llnl.gov/projects/hypre-scalable-linear-solvers-multigrid-methods/download/hypre-2.11.2.tar.gz\u201d -o \u201chypre-2.11.2.tar.gz\u201d ( from Jen the above is the correct url for the hypre installation) >> gunzip -c hypre-2.11.2.tar.gz | tar xf - mv hypre-2.11.2 hypre` (That would vary if the version number changes, and your HYPRE_DIR variable would need to reflect your version/directory names). Now onto the setup. >> cd ~/ParF/ hypre/src >> ./configure --prefix=$HYPRE_DIR >> make install From Jen: If make install is not working and you receive a Fatal error mpi.h is not found: go into your bash_profile and change the two CC and CXX lines to the normal mpi complier export PARFLOW_DIR=~/ParF/parflow export HYPRE_DIR=~/ParF/hypre export SILO_DIR=~/ParF/silo export CC=mpicc export CXX=mpic++ export FC=gfortran export F77=gfortran Once done: Recompile via .configure --prefix=$HYPRE_DIR Then make install This should correct the previous error and allow you to install This installation also takes a while to install, but it\u2019s usually not as long as OpenMPI. You\u2019re likely to see a lot of warnings about \u201ccould not find any symbols\u201d but as long as there are no errors you should be able to move on to ParFlow. Step 6) Installing ParFlow This step is where we depart significantly from the previous installation instructions, but in a good way. The ParFlow build process is now more streamlined because it uses CMake. Instead of separate builds for pfsimulator (the model itself) and pftools (the controlling interface), there is one unified build process. There are a lot of different setup options for ParFlow but for our \u201ctypical\u201d setup we\u2019re assuming you want to run in parallel, you might want to run CLM, you have Silo and Hypre, and a few other little things. First, we need to download the code. ParFlow is hosted on GitHub, so to obtain the code: >> cd ~/ParF >> git clone https://github.com/parflow/parflow.git --branch master --single-branch_ which will create a folder called parflow in ParF and now you\u2019ve got the source code. You may want to rename the folder (I like to use the date I downloaded it so /parflow.20190209 for example) but you\u2019ll need to update your PARFLOW_DIR too if you change that. I\u2019m going to proceed assuming you have not renamed it. Next, we\u2019ll create a directory for the incremental files of the build process called \u201cbuild\u201d >> mkdir build >> cd build The next step will tell CMake how to configure ParFlow using the typical options (copy and paste will be your friend on this): >> cmake ../parflow -DCMAKE_INSTALL_PREFIX=$PARFLOW_DIR -DHYPRE_ROOT=$HYPRE_DIR -DPARFLOW_AMPS_LAYER=mpi1 -DPARFLOW_AMPS_SEQUENTIAL_IO=true -DPARFLOW_ENABLE_TIMING=true -DSILO_ROOT=$SILO_DIR -DPARFLOW_HAVE_CLM=ON Note that if your parflow source tree is NOT in a folder called parflow, you\u2019ll need to change that right after \u201ccmake\u201d in the above command. The manual provides a few extra details on how you can customize an installation using the \u201cccmake\u201d interface but we don\u2019t need that now. Next type: >> make and lots of stuff will happen. If the last two lines resemble: [100%] Linking C shared library libpftools.dylib [100%] Built target pftools Then you\u2019re ready for the last command: >> make install The difference is that \u201cmake\u201d builds everything in our temporary structure (the reason we created that build directory in the first), and \u201cmake install\u201d copies the final build into our installation location. On a few computers we\u2019ve found that after the \u201cmake\u201d command you might see something like this: collect2: error: ld returned 1 exit status make[2]: *** [pftools/libpftools.dylib] Error 1 make[1]: *** [pftools/CMakeFiles/pftools.dir/all] Error 2 make: *** [all] Error 2 If that\u2019s the case, just back up and add \u201c-DCMAKE_SHARED_LINKER_FLAGS=-ltcl8.5\u201d to the end of that list for the \u201ccmake\u201d command, the \u201cmake\u201d again and you should be all sorted out. And that\u2019s it for the installation. The last step is to test the installation. Navigate to: >> cd ~/ParF/parflow/test {Or wherever you installed to} Then type: >> make check A lot of things will fly by as ParFlow verifies that it is working correctly. Most should pass, but some will likely fail if you have fewer processors than are being tested; for example, some of the tests need nine processors and these will fail on a system with only 4. You\u2019ll see a report at the end and as long as most pass, you\u2019re all done and ParFlow has been installed successfully. Step 7) Some troubleshooting tips The single most common reason steps of the installation don\u2019t go well is not being in the correct path. You can always verify your location in the directory tree using the \u201cpwd\u201d command and each step tells you where you should be before you begin entering commands. You mac is also case-sensitive so pay attention to capitalization. That also tends to be a wrench in the gears of the setup process. If you prefer to use the c-shell instead of the bash-shell most of the steps are the same but your environment variables are defined differently. Fortunately, the setup example in the ParFlow manual outlines the c-shell variations of all the steps we covered here. Lastly, if you have MacPorts or any other \u201chelper packages\u201d your system paths may have been altered from the defaults. Defining the environment variables at the bottom of your .bash_profile should take care of this. If you\u2019ve installed mpi properly but got errors with mpi, one solution can be turn off WIFI and keep WIFI off when running ParFLow. Step 8) What to do next See that wasn\u2019t nearly as bad as you thought it was going to be, so now what? Well, you could start getting to work with ParFlow (which is totally what you should do if you\u2019re a grad student), but maybe go outside, enjoy some fresh air, and see how things are today first. Maybe take a break and have a tasty beverage of your choosing while you ponder the intricacies of the scientific awesomeness you\u2019re going to unleash upon the world using ParFlow. MacOS High Sierra Notes from Laura: 9/11/2018 Summary: Successful install: With GCC 8.1.0, MPI 3.1.2, Hypre 2.9.0b, Slio 4.10.2 Only modification from PF Blog is that I compiled MPI with clang per Nick\u2019s instructions (see the configure line below). Also it didn\u2019t work when I ran tests with WiFI on when I was at school (MPI error) but it worked once I turned it off. Edited the bash profile per the instructions in the Yosemite blog post: http://parflow.blogspot.com/2014/10/installing-parflow-in-os-x-yosemite.html Installed the latest GCC per blog instructions: http://hpc.sourceforge.net/ After this it still wasn\u2019t pointing to the correct gcc but this was fixed by a restart dhcp-10-134-192-214:~ laura$ gcc --version gcc (GCC) 8.1.0 Copyright (C) 2018 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. dhcp-10-134-192-214:~ laura$ gfortran --version GNU Fortran (GCC) 8.1.0 Copyright (C) 2018 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Next I got the latest MPI: https://www.open-mpi.org/software/ompi/v3.1/ tar -xvf openmpi-3.1.2.tar.gz cd openmpi-3.1.2 ./configure --prefix=/usr/local/ CC=/usr/bin/clang CXX=/usr/bin/clang cd openmpi-3.1.2 sudo make all install ompi_info mpiexec \u2014version mpirun \u2014version Get Parflow: git clone https://github.com/parflow/parflow mv parflow/ parflow.git Get Silo: https://wci.llnl.gov/simulation/computer-codes/silo/downloads Follow instructions from ParFlow Blog to build: > cd silo > ./configure --disable-silex > make install > cd .. Get Hypre 2.9.0b: https://computation.llnl.gov/projects/hypre-scalable-linear-solvers-multigrid-methods/software Follow instructions from ParFlow Blog to build: > tar -xvf hypre-2.9.0b.tar.gz > mv hypre-2.9.0b hypre > cd hypre/src > ./configure --prefix=$HYPRE_DIR > make install Followed steps 7 and 8 from the blog to install par flow ( http://parflow.blogspot.com/2014/10/installing-parflow-in-os-x-yosemite.html ) All tests were failing with an MPI error when I had wifi turned on but when I turned it off they all pass Other People\u2019s Notes on High Seirra: From Nick: For MPI: Version 3.0.0 Two catches to this though. 1) I had to use clang, and 2) I specified the path and compilers manually. Here\u2019s the exact line I used:./configure --prefix=/usr/local/ CC=/usr/bin/clang CXX=/usr/bin/clang(FYI you can verify your MPI version with \u201compi_info\u201d at the command line and the version will be at the top) Next Hypre/Silo: It\u2019s Hypre 2.9.0b and Silo 2.9.1. I haven\u2019t tried any of the other versions lately, BUT these were installed with gcc, looks like v4.9.0 according to the config log And PF was built with the same GCC and gfortran version used for hypre. I had no luck getting PF to build with clang so, annoyingly, it\u2019s exactly the opposite of openMPI. From Reed: /Users/reed/parflow/libraries> gcc \u2014version gcc (GCC) 5.0.0 20141005 (experimental) Copyright (C) 2014 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. /Users/reed/parflow/libraries> gfortran \u2014version GNU Fortran (GCC) 5.0.0 20141005 (experimental) Copyright (C) 2014 Free Software Foundation, Inc. GNU Fortran comes with NO WARRANTY, to the extent permitted by law. You may redistribute copies of GNU Fortran under the terms of the GNU General Public License. For more information about these matters, see the file named COPYING PFNetCDF on TACC Stampede2 Katie Markovich, March 28, 2019 Home directory = /home1/06175/khm293 [make this your home directory!] .**bashrc** module load git module load gcc export CC=gcc export CXX=g++ export FC=gfortran export F77=gfortran export NCDIR=/home1/06175/khm293/ParF/netcdf4 export H5DIR=/home1/06175/khm293/ParF/hdf5 export MPI_DIR=/home1/06175/khm293/ParF/OMP-4.0.0 export MPI_RUN_DIR=/home1/06175/khm293/ParF/OMP-4.0.0/bin export HYPRE_DIR=/home1/06175/khm293/ParF/hypre export SILO_DIR=/home1/06175/khm293/ParF/silo export PARFLOW_DIR=/home1/06175/khm293/ParF/parflow PATH=$PATH:$PARFLOW_DIR/bin **hdf5** Download hdf5 here: https://www.hdfgroup.org/downloads/ >> cd ~ >> mkdir ParF >> cd ParF >>mkdir hdf5 >>exportH5DIR=${HOME}/ParF/hdf5 >>gunzip hdf5-1.10.4.tar.gz >>tar -xvf hdf5-1.10.4.tar >>cd hdf5-1.10.4/ >>CC=mpicc ./configure --enable-parallel --prefix=${H5DIR} >>make >>make install >>cd .. netCDF4 Download netCDF4 here: https://www.unidata.ucar.edu/downloads/netcdf/index.jsp >>mkdir ${HOME}/ParF/netcdf4 >>exportNCDIR=${HOME}/ParF/netcdf4 >>gunzip netcdf-4.6.2.tar.gz >>tar -xvf netcdf-4.6.2.tar >>cd netcdf-4.6.2/ >>CC=mpicc CPPFLAGS=-I${H5DIR}/include LDFLAGS=-L${H5DIR}/lib \\ ./configure --disable-shared--disable-dap--enable-parallel-tests --prefix=${NCDIR} >>make >>make install From Nick\u2019s post: openMPI >> cd ~ >> cd ParF >> mkdir OMPI-4.0.0 >> curl \"https://download.open-mpi.org/release/open-mpi/v4.0/openmpi-4.0.0.tar.gz\" -o \"openmpi-4.0.0.tar.gz\" >> gunzip -c openmpi-4.0.0.tar.gz | tar xf - >> cd openmpi-4.0.0 >> ./configure --prefix=$MPI_DIR >> make >> make install Silo >> cd ~/ParF >> curl \"https://wci.llnl.gov/content/assets/docs/simulation/computer-codes/silo/silo-4.10.2/silo-4.10.2.tar.gz\" -o \"silo-4.10.2.tar.gz\" >> gunzip -c silo-4.10.2.tar.gz | tar xf - >> mv silo-4.10.2 silo >> cd silo >> ./configure --disable-silex >> make install Hypre >> cd .. >> cd ~/ParF >> curl \"https://computation.llnl.gov/projects/hypre-scalable-linear-solvers-multigrid-methods/download/hypre-2.11.2.tar.gz\" -o \"hypre-2.11.2.tar.gz\" >> gunzip -c hypre-2.11.2.tar.gz | tar xf \u2013 >> mv hypre-2.11.2 hypre >> cd ~/ParF/ hypre/src >> ./configure --prefix=$HYPRE_DIR >> make install Parflow-NetCDF >> cd ~/ParF >> git clone https://github.com/parflow/parflow.git --branch master --single-branch >> mkdir build >> cd build >>cmake ../parflow -DCMAKE_INSTALL_PREFIX=$PARFLOW_DIR -DHYPRE_ROOT=$HYPRE_DIR -DPARFLOW_AMPS_LAYER=mpi1 -DPARFLOW_AMPS_SEQUENTIAL_IO=true -DPARFLOW_ENABLE_TIMING=true -DSILO_ROOT=$SILO_DIR -DPARFLOW_HAVE_CLM=ON -DNETCDF_DIR=$NCDIR -DHDF5_ROOT=$H5DIR","title":"ParFlow Install"},{"location":"how_to_notes/HPC_systems/parflow_install/#parflow-install","text":"Laura Condon, 2/13/19","title":"ParFlow Install"},{"location":"how_to_notes/HPC_systems/parflow_install/#description","text":"A running set of notes on ParFlow installs for various platforms. Please update this with your experiences either by adding comments or inserting text. Please Add new installs to the top of this doc and use headings so that they can be included in the table of contents. **NOTE: If you are installing on a mac refer first to the ParFlow blog http://parflow.blogspot.com/ and look for the latest version in the compiling section which corresponds to your operating system. ** Links: You should also refer to the ParFlow blog for additional resources http://parflow.blogspot.com/","title":"Description"},{"location":"how_to_notes/HPC_systems/parflow_install/#ua-hpc-ocelote","text":"Notes from Laura Fall 2018","title":"UA-HPC Ocelote"},{"location":"how_to_notes/HPC_systems/parflow_install/#contacts","text":"Ric Anderson: ric@email.arizona.edu Chris Reidy: chrisreidy@email.arizona.edu","title":"Contacts"},{"location":"how_to_notes/HPC_systems/parflow_install/#software-install-request-form","text":"https://it.arizona.edu/service-request-forms They built Hypre and silo and these are available as modules","title":"Software install request form"},{"location":"how_to_notes/HPC_systems/parflow_install/#module-commands","text":"https://docs.hpc.arizona.edu/display/UAHPC/Accessing+Software Module list (gives loaded modules) Module avail (gives list of all modules) Module show NAME (gives info on a specific module","title":"Module commands"},{"location":"how_to_notes/HPC_systems/parflow_install/#job-submit-and-queue-information","text":"Ocelote uses pbs for its job management system. Example pbs script for running on 1 node: ### Your job will use 1 node, 28 cores, and 168gb of memory total. #PBS -q standard #PBS -l select=1:ncpus=28:mem=168gb:pcmem=6gb ### Specify a name for the job #PBS -N job_name ### Specify the group name #PBS -W group_list=lecondon ### Walltime is how long your job will run #PBS -l walltime=00:50:00 Example pbs script for debugging script (always a good idea before you run anything): ### Your job will use 1 node, 28 cores, and 168gb of memory total. #PBS -q debug #PBS -l select=1:ncpus=28:mem=168gb:pcmem=6gb ### Specify a name for the job #PBS -N job_name ### Specify the group name #PBS -W group_list=lecondon ### Walltime is how long your job will run #PBS -l walltime=00:05:00 qsub job_name.pbs (submits jobs) qstat (lists all jobs running and in queue)","title":"Job submit and queue information"},{"location":"how_to_notes/HPC_systems/parflow_install/#building-parflow","text":"ssh_ _username@hpc.arizona.edu Download ParFlow git clone https://github.com/parflow/parflow Rename this directory to whatever you want Setup environment vi ~/.bashrc Add the following lines to the user specified portion: module load gcc module load hypre module load silo export PARFLOW_DIR=/home/PATH_TO_YOUR_PARFLOW_DIR PATH=$PATH:$PARFLOW_DIR/bin source ~/.bashrc","title":"Building ParFlow"},{"location":"how_to_notes/HPC_systems/parflow_install/#build-parflow","text":"cd pfsimulator ./configure --prefix=$PARFLOW_DIR --with-amps=mpi1 --with-clm --with-hypre=$HYPRE_BASE --with-silo=$SILO_BASE --with-amps-sequential-io make -j 14 make install Build PFtools cd ../pftools ./configure --prefix=$PARFLOW_DIR --with-amps=mpi1 --with-clm --with-hypre=$HYPRE_BASE --with-silo=$SILO_BASE --with-amps-sequential-io make -j 14 make install Run tests cd ../test make check","title":"Build ParFlow"},{"location":"how_to_notes/HPC_systems/parflow_install/#macos-mojave","text":"These notes are from Nick Engdahl 2/12i/2019: This post will describe a \u201ctypical\u201d ParFlow installation, that is one that uses the most common options and configurations on macOS using the default bash shell. We\u2019ll primarily use the command line for this so go ahead and open a terminal window (if you don\u2019t know where terminal is use spotlight search).","title":"MacOS Mojave"},{"location":"how_to_notes/HPC_systems/parflow_install/#preliminaries","text":"For commands entered into the terminal window I\u2019ll use the following convention where the command is preceded with \u201c >> \u201d for example: >> make {and here\u2019s an example comment/note you don\u2019t type} Anything that follows in curly brackets { } is just a note or comment and is NOT to be entered. OK, first thing\u2019s first. We need to define a few environment variables for the installation. Navigate to your home folder and create (or edit) your bash_profile: >> cd ~ >> vim .bash_profile In vim, hit \u201ci\u201d to enter insert mode and add the following lines, preferably at the bottom: export PARFLOW_DIR=~/ParF/parflow export HYPRE_DIR=~/ParF/hypre export SILO_DIR=~/ParF/silo export CC=clang export CXX=clang++ export FC=gfortran export F77=gfortran then press \u201cescape\u201d followed by \u201c:\u201d then \u201cx\u201d to save the changes and exit. To apply the changes: >> source .bash_profile This has now defined some environment variables and shortcuts we\u2019ll take advantage of later on. Next, we\u2019ll create a directory to hold all the various bits that comprise a typical installation. From your home directory, make a folder called \u201cParF\u201d and enter it: >> cd ~ {only necessary if you\u2019ve navigated out of home} >> mkdir ParF >> cd ParF There are 6 \u201cparts\u201d to these instructions and we\u2019re going to handle each separately: 1) install gcc/gfortran, 2) install cmake, 3) install OpenMPI, 4) Install Silo, 5) Install Hypre, 6) install ParFlow. Everything except step 6 are supporting libraries or programs that ParFlow needs for our typical installation. Step 1) Installing gfortran and gcc Truth be told we only really need gfortran but gcc is nice to have, and since they\u2019re often bundled it makes sense to knock both out. You could try building them from source code, but the easier way is to use a binary that is pre-built for MacOS. Those can be downloaded here: http://hpc.sourceforge.net The option you want is usually the first entry right below the word \u201cBinaries\u201d after the \u201cbig\u201d paragraph. Today, for me that is \u201cgcc-8.1-bin.tar.gz\u201d and clicking on that link put it in my \u201cDownloads\u201d folder. You don\u2019t need to bother with the \u201cgfortran\u201d link because it\u2019s already included in this one. In your terminal window, we first need to enable compiling programs on your mac. This is necessary after every major MacOS upgrade before you can compile new programs (old one will be fine though). >> sudo xcode-select --install A confirmation window will likely open and after you OK through all of that, we\u2019re done with that. You might also see a message saying command line tools are already installed, which is also fine. Next, navigate to wherever you put that download and extract it to /usr/local with: >> cd ~/Downloads {assuming your Download went here} >> sudo tar -xvf gcc-8.1-bin.tar -C / A lot of text will fly by then you\u2019ll get a command prompt. To make sure this has copied correctly, at the command prompt type: >> gfortran -v And you should get something like this: Using built-in specs. COLLECT_GCC=gfortran COLLECT_LTO_WRAPPER=/usr/local/libexec/gcc/x86_64-apple-darwin17.5.0/8.1.0/lto-wrapper Target: x86_64-apple-darwin17.5.0 Configured with: ../gcc-8.1.0/configure --enable-languages=c++,fortran Thread model: posix gcc version 8.1.0 (GCC) There is one last critically important step we need to take that was not necessary in the past. We need to install the package in MacOS that has some of the libraries we need in order to compile programs. These are located in \u201c/Library/Developer/CommandLineTools/Packages/\u201d and the easiest way to get there is to open a finder window, press command+shift+g, then paste in that path. There is only one package in this folder (macOS_SDK_headers_for_macOS_10.14.pkg), though its name might change, and to install it just double click on it and follow the instructions. I would also suggest you run \u201csudo --xcode-select --s /Library/Developer/CommandLineTools\u201d to make sure the system is pointing to the right version. Apple may be depreciating the package in future macOS releases, but we\u2019ll update you if/when that happens. Step 2) Installing cmake Past installation instructions have used the GNU autoconfig system. We\u2019ll still use that for some of the supporting components, but ParFlow itself is now built using the CMake interface. This isn\u2019t installed on your mac by default, so we\u2019ll start by adding it. We\u2019re going to download the latest version of the CMake source. There is also a GUI for CMake but we\u2019re not going to use it. Before proceeding, check here: https://cmake.org/download/ for the latest version. On the download page, go ahead and click on the link for the archive in the Unix/Linux source box and the download should complete shortly. Assuming the archive is in your ~/Downloads/ folder, we\u2019ll copy it into ParF, the expand the archive: As of this writing that file is \u201ccmake-3.14.0-rc1.tar.gz\u201d To move that download directly into the ParF folder, in your terminal window, be sure you\u2019re in the ParF directory, then type: >> cd ~/ParF/ >> cp ~/Downloads/cmake-3.14.0-rc1.tar . >> tar -xvf cmake-3.14.0-rc1.tar Note the period at the end of the second line. You will need to update the file name as newer versions of CMake are released. Now we\u2019ll enter the new directory and start building using cmake\u2019s helper: >> cd cmake-3.14.0-rc1 >> ./bootstrap {This will take a while} >> make {This will also take a while} >> sudo make install _ At the end of the make process you\u2019ll see percentage signs on the left edge of your screen and if it makes it to 100 without errors, you\u2019re all set. *From Jun: When installing cmake , on the step of ./bootstrap , sometimes it will pop up an error message that \u2018Unable to find any JVMs matching version\u2019, just follow the instruction to install the up-to-date JAVA\u2019 Step 3) Installing OpenMPI OpenMPI is the message passing interface we use for ParFlow (note that this is not the same as OpenMP). This is also what makes it parallel so we\u2019ll do this step next. Navigate back to your ParF directory: >> cd ~/ParF Next we\u2019re going to download openmpi. The simplest way to do that is from the command line using the \u201ccurl\u201d command, which is like \u201cwget\u201d for a mac. However, before doing so you might want to check to make sure you\u2019re downloading the most recent version at \u201chttps://www.open-mpi.org/software/ompi/\u201d You\u2019ll need to modify the version numbers if using a more recent release but otherwise it\u2019ll be just like this: >> curl \u201chttps://download.open-mpi.org/release/open-mpi/v4.0/openmpi-4.0.0.tar.gz\u201d -o \u201copenmpi-4.0.0.tar.gz\u201d (Note: When copying and pasting, sometimes quotes get messed up. If you get an error from that command about libcurl, just delete then re-add the quotation marks manually once you\u2019re in the terminal window.) Next, we\u2019ll decompress and extract the archive in one step: >> gunzip -c openmpi-4.0.0.tar.gz | tar xf - Next, we\u2019ll configure the installation package for our system. Navigate into the newly created openmpi-4.0.0 folder and type: >> ./configure --prefix=/usr/local CC=/usr/bin/clang CXX=/usr/bin/clang++ Those CC and CXX arguments force the computer to use a specific compiler, which is clang. On most systems this should install fine without those, but it doesn\u2019t hurt to have them. After you hit return, lots of stuff will fly by your screen for a long time. Once that\u2019s done, type: >> sudo make all install which will take forever and a decade before it completes, but then that\u2019s it; OpenMPI is now installed. If you want to check, type \u201cmpirun\u201d at the command line, but not much will happen since it can\u2019t find anything to do. Important: Remember that sudo is the \u201cnuclear option\u201d on unix-esque computers and should only be used when absolutely necessary; the install path for OpenMPI is inside a protected folder so we need to invoke sudo in this case, otherwise the installation can\u2019t go to the default. You could specify an alternate location, but since most applications that might use OpenMPI will look here, we might as well. Step 4) Installing Silo Silo is a file format that is used as the default for viewing ParFlow outputs, but we also now have other options available. Many of the test cases use it so we always recommend installing it. The latest version of silo can be found at: https://wci.llnl.gov/simulation/computer-codes/silo/downloads You could download the archive in a web browser and move it but instead I\u2019m going to use curl again. Navigate to the ParF folder and the commands are: >> cd ~/ParF >> curl \u201chttps://wci.llnl.gov/content/assets/docs/simulation/computer-codes/silo/silo-4.10.2/silo-4.10.2.tar.gz\u201d -o \u201csilo-4.10.2.tar.gz\u201d Mimicking the process we used for OpenMPI, to expand and extract the archive type: >> gunzip -c silo-4.10.2.tar.gz | tar xf - Now we\u2019ll rename the newly expanded directory to match our environment variable SILO_DIR: >> mv silo-4.10.2 silo Now we\u2019ll navigate into the new directory, configure, and install: >> cd silo >> ./configure --disable-silex >> make install >> cd .. And that completes the installation. Note that if your version of Silo differs, you\u2019ll need to update the appropriate file/directory names environment variable we set to reflect your installation path. Step 5) Installing the HYPRE library HYPRE is a library of numerical solvers for high-performance computing applications and ParFlow hooks into it for some of its functionality. As before the simplest way to get this is curl. There were some issues with hypre version 10 and ParFlow but the current release (2.11.2) or newer should work just fine. Installing this is nothing more than a minor variation of what we\u2019ve already done: >> cd ~/ParF >> curl \u201chttps://computating.llnl.gov/projects/hypre-scalable-linear-solvers-multigrid-methods/download/hypre-2.11.2.tar.gz\u201d -o \u201chypre-2.11.2.tar.gz\u201d ( from Jen the above is the correct url for the hypre installation) >> gunzip -c hypre-2.11.2.tar.gz | tar xf - mv hypre-2.11.2 hypre` (That would vary if the version number changes, and your HYPRE_DIR variable would need to reflect your version/directory names). Now onto the setup. >> cd ~/ParF/ hypre/src >> ./configure --prefix=$HYPRE_DIR >> make install From Jen: If make install is not working and you receive a Fatal error mpi.h is not found: go into your bash_profile and change the two CC and CXX lines to the normal mpi complier export PARFLOW_DIR=~/ParF/parflow export HYPRE_DIR=~/ParF/hypre export SILO_DIR=~/ParF/silo export CC=mpicc export CXX=mpic++ export FC=gfortran export F77=gfortran Once done: Recompile via .configure --prefix=$HYPRE_DIR Then make install This should correct the previous error and allow you to install This installation also takes a while to install, but it\u2019s usually not as long as OpenMPI. You\u2019re likely to see a lot of warnings about \u201ccould not find any symbols\u201d but as long as there are no errors you should be able to move on to ParFlow. Step 6) Installing ParFlow This step is where we depart significantly from the previous installation instructions, but in a good way. The ParFlow build process is now more streamlined because it uses CMake. Instead of separate builds for pfsimulator (the model itself) and pftools (the controlling interface), there is one unified build process. There are a lot of different setup options for ParFlow but for our \u201ctypical\u201d setup we\u2019re assuming you want to run in parallel, you might want to run CLM, you have Silo and Hypre, and a few other little things. First, we need to download the code. ParFlow is hosted on GitHub, so to obtain the code: >> cd ~/ParF >> git clone https://github.com/parflow/parflow.git --branch master --single-branch_ which will create a folder called parflow in ParF and now you\u2019ve got the source code. You may want to rename the folder (I like to use the date I downloaded it so /parflow.20190209 for example) but you\u2019ll need to update your PARFLOW_DIR too if you change that. I\u2019m going to proceed assuming you have not renamed it. Next, we\u2019ll create a directory for the incremental files of the build process called \u201cbuild\u201d >> mkdir build >> cd build The next step will tell CMake how to configure ParFlow using the typical options (copy and paste will be your friend on this): >> cmake ../parflow -DCMAKE_INSTALL_PREFIX=$PARFLOW_DIR -DHYPRE_ROOT=$HYPRE_DIR -DPARFLOW_AMPS_LAYER=mpi1 -DPARFLOW_AMPS_SEQUENTIAL_IO=true -DPARFLOW_ENABLE_TIMING=true -DSILO_ROOT=$SILO_DIR -DPARFLOW_HAVE_CLM=ON Note that if your parflow source tree is NOT in a folder called parflow, you\u2019ll need to change that right after \u201ccmake\u201d in the above command. The manual provides a few extra details on how you can customize an installation using the \u201cccmake\u201d interface but we don\u2019t need that now. Next type: >> make and lots of stuff will happen. If the last two lines resemble: [100%] Linking C shared library libpftools.dylib [100%] Built target pftools Then you\u2019re ready for the last command: >> make install The difference is that \u201cmake\u201d builds everything in our temporary structure (the reason we created that build directory in the first), and \u201cmake install\u201d copies the final build into our installation location. On a few computers we\u2019ve found that after the \u201cmake\u201d command you might see something like this: collect2: error: ld returned 1 exit status make[2]: *** [pftools/libpftools.dylib] Error 1 make[1]: *** [pftools/CMakeFiles/pftools.dir/all] Error 2 make: *** [all] Error 2 If that\u2019s the case, just back up and add \u201c-DCMAKE_SHARED_LINKER_FLAGS=-ltcl8.5\u201d to the end of that list for the \u201ccmake\u201d command, the \u201cmake\u201d again and you should be all sorted out. And that\u2019s it for the installation. The last step is to test the installation. Navigate to: >> cd ~/ParF/parflow/test {Or wherever you installed to} Then type: >> make check A lot of things will fly by as ParFlow verifies that it is working correctly. Most should pass, but some will likely fail if you have fewer processors than are being tested; for example, some of the tests need nine processors and these will fail on a system with only 4. You\u2019ll see a report at the end and as long as most pass, you\u2019re all done and ParFlow has been installed successfully. Step 7) Some troubleshooting tips The single most common reason steps of the installation don\u2019t go well is not being in the correct path. You can always verify your location in the directory tree using the \u201cpwd\u201d command and each step tells you where you should be before you begin entering commands. You mac is also case-sensitive so pay attention to capitalization. That also tends to be a wrench in the gears of the setup process. If you prefer to use the c-shell instead of the bash-shell most of the steps are the same but your environment variables are defined differently. Fortunately, the setup example in the ParFlow manual outlines the c-shell variations of all the steps we covered here. Lastly, if you have MacPorts or any other \u201chelper packages\u201d your system paths may have been altered from the defaults. Defining the environment variables at the bottom of your .bash_profile should take care of this. If you\u2019ve installed mpi properly but got errors with mpi, one solution can be turn off WIFI and keep WIFI off when running ParFLow. Step 8) What to do next See that wasn\u2019t nearly as bad as you thought it was going to be, so now what? Well, you could start getting to work with ParFlow (which is totally what you should do if you\u2019re a grad student), but maybe go outside, enjoy some fresh air, and see how things are today first. Maybe take a break and have a tasty beverage of your choosing while you ponder the intricacies of the scientific awesomeness you\u2019re going to unleash upon the world using ParFlow.","title":"Preliminaries"},{"location":"how_to_notes/HPC_systems/parflow_install/#macos-high-sierra","text":"Notes from Laura: 9/11/2018 Summary: Successful install: With GCC 8.1.0, MPI 3.1.2, Hypre 2.9.0b, Slio 4.10.2 Only modification from PF Blog is that I compiled MPI with clang per Nick\u2019s instructions (see the configure line below). Also it didn\u2019t work when I ran tests with WiFI on when I was at school (MPI error) but it worked once I turned it off. Edited the bash profile per the instructions in the Yosemite blog post: http://parflow.blogspot.com/2014/10/installing-parflow-in-os-x-yosemite.html Installed the latest GCC per blog instructions: http://hpc.sourceforge.net/ After this it still wasn\u2019t pointing to the correct gcc but this was fixed by a restart dhcp-10-134-192-214:~ laura$ gcc --version gcc (GCC) 8.1.0 Copyright (C) 2018 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. dhcp-10-134-192-214:~ laura$ gfortran --version GNU Fortran (GCC) 8.1.0 Copyright (C) 2018 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Next I got the latest MPI: https://www.open-mpi.org/software/ompi/v3.1/ tar -xvf openmpi-3.1.2.tar.gz cd openmpi-3.1.2 ./configure --prefix=/usr/local/ CC=/usr/bin/clang CXX=/usr/bin/clang cd openmpi-3.1.2 sudo make all install ompi_info mpiexec \u2014version mpirun \u2014version Get Parflow: git clone https://github.com/parflow/parflow mv parflow/ parflow.git Get Silo: https://wci.llnl.gov/simulation/computer-codes/silo/downloads Follow instructions from ParFlow Blog to build: > cd silo > ./configure --disable-silex > make install > cd .. Get Hypre 2.9.0b: https://computation.llnl.gov/projects/hypre-scalable-linear-solvers-multigrid-methods/software Follow instructions from ParFlow Blog to build: > tar -xvf hypre-2.9.0b.tar.gz > mv hypre-2.9.0b hypre > cd hypre/src > ./configure --prefix=$HYPRE_DIR > make install Followed steps 7 and 8 from the blog to install par flow ( http://parflow.blogspot.com/2014/10/installing-parflow-in-os-x-yosemite.html ) All tests were failing with an MPI error when I had wifi turned on but when I turned it off they all pass Other People\u2019s Notes on High Seirra: From Nick: For MPI: Version 3.0.0 Two catches to this though. 1) I had to use clang, and 2) I specified the path and compilers manually. Here\u2019s the exact line I used:./configure --prefix=/usr/local/ CC=/usr/bin/clang CXX=/usr/bin/clang(FYI you can verify your MPI version with \u201compi_info\u201d at the command line and the version will be at the top) Next Hypre/Silo: It\u2019s Hypre 2.9.0b and Silo 2.9.1. I haven\u2019t tried any of the other versions lately, BUT these were installed with gcc, looks like v4.9.0 according to the config log And PF was built with the same GCC and gfortran version used for hypre. I had no luck getting PF to build with clang so, annoyingly, it\u2019s exactly the opposite of openMPI. From Reed: /Users/reed/parflow/libraries> gcc \u2014version gcc (GCC) 5.0.0 20141005 (experimental) Copyright (C) 2014 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. /Users/reed/parflow/libraries> gfortran \u2014version GNU Fortran (GCC) 5.0.0 20141005 (experimental) Copyright (C) 2014 Free Software Foundation, Inc. GNU Fortran comes with NO WARRANTY, to the extent permitted by law. You may redistribute copies of GNU Fortran under the terms of the GNU General Public License. For more information about these matters, see the file named COPYING","title":"MacOS High Sierra"},{"location":"how_to_notes/HPC_systems/parflow_install/#pfnetcdf-on-tacc-stampede2","text":"Katie Markovich, March 28, 2019 Home directory = /home1/06175/khm293 [make this your home directory!] .**bashrc** module load git module load gcc export CC=gcc export CXX=g++ export FC=gfortran export F77=gfortran export NCDIR=/home1/06175/khm293/ParF/netcdf4 export H5DIR=/home1/06175/khm293/ParF/hdf5 export MPI_DIR=/home1/06175/khm293/ParF/OMP-4.0.0 export MPI_RUN_DIR=/home1/06175/khm293/ParF/OMP-4.0.0/bin export HYPRE_DIR=/home1/06175/khm293/ParF/hypre export SILO_DIR=/home1/06175/khm293/ParF/silo export PARFLOW_DIR=/home1/06175/khm293/ParF/parflow PATH=$PATH:$PARFLOW_DIR/bin **hdf5** Download hdf5 here: https://www.hdfgroup.org/downloads/ >> cd ~ >> mkdir ParF >> cd ParF >>mkdir hdf5 >>exportH5DIR=${HOME}/ParF/hdf5 >>gunzip hdf5-1.10.4.tar.gz >>tar -xvf hdf5-1.10.4.tar >>cd hdf5-1.10.4/ >>CC=mpicc ./configure --enable-parallel --prefix=${H5DIR} >>make >>make install >>cd .. netCDF4 Download netCDF4 here: https://www.unidata.ucar.edu/downloads/netcdf/index.jsp >>mkdir ${HOME}/ParF/netcdf4 >>exportNCDIR=${HOME}/ParF/netcdf4 >>gunzip netcdf-4.6.2.tar.gz >>tar -xvf netcdf-4.6.2.tar >>cd netcdf-4.6.2/ >>CC=mpicc CPPFLAGS=-I${H5DIR}/include LDFLAGS=-L${H5DIR}/lib \\ ./configure --disable-shared--disable-dap--enable-parallel-tests --prefix=${NCDIR} >>make >>make install From Nick\u2019s post: openMPI >> cd ~ >> cd ParF >> mkdir OMPI-4.0.0 >> curl \"https://download.open-mpi.org/release/open-mpi/v4.0/openmpi-4.0.0.tar.gz\" -o \"openmpi-4.0.0.tar.gz\" >> gunzip -c openmpi-4.0.0.tar.gz | tar xf - >> cd openmpi-4.0.0 >> ./configure --prefix=$MPI_DIR >> make >> make install Silo >> cd ~/ParF >> curl \"https://wci.llnl.gov/content/assets/docs/simulation/computer-codes/silo/silo-4.10.2/silo-4.10.2.tar.gz\" -o \"silo-4.10.2.tar.gz\" >> gunzip -c silo-4.10.2.tar.gz | tar xf - >> mv silo-4.10.2 silo >> cd silo >> ./configure --disable-silex >> make install Hypre >> cd .. >> cd ~/ParF >> curl \"https://computation.llnl.gov/projects/hypre-scalable-linear-solvers-multigrid-methods/download/hypre-2.11.2.tar.gz\" -o \"hypre-2.11.2.tar.gz\" >> gunzip -c hypre-2.11.2.tar.gz | tar xf \u2013 >> mv hypre-2.11.2 hypre >> cd ~/ParF/ hypre/src >> ./configure --prefix=$HYPRE_DIR >> make install Parflow-NetCDF >> cd ~/ParF >> git clone https://github.com/parflow/parflow.git --branch master --single-branch >> mkdir build >> cd build >>cmake ../parflow -DCMAKE_INSTALL_PREFIX=$PARFLOW_DIR -DHYPRE_ROOT=$HYPRE_DIR -DPARFLOW_AMPS_LAYER=mpi1 -DPARFLOW_AMPS_SEQUENTIAL_IO=true -DPARFLOW_ENABLE_TIMING=true -DSILO_ROOT=$SILO_DIR -DPARFLOW_HAVE_CLM=ON -DNETCDF_DIR=$NCDIR -DHDF5_ROOT=$H5DIR","title":"PFNetCDF on TACC Stampede2"},{"location":"how_to_notes/HPC_systems/verde/","text":"Coming Soon!","title":"Verde"},{"location":"how_to_notes/data_and_file_transfers/apple_remote_desktop_ssh/","text":"Remote Desktop / SSH / Basic Command line Amanda Triplett 10/2019 Description: Accessing your computer via SSH through your command line with a general example of how to create a TAR file remotely and move it to your home computer. It also outlines how to remotely manage your desktop from mac to mac computer. Method 1a: From Apple to Apple: SSH Open a command line and type ssh \u201cyour_username\u201d@\u201dyour_static_IP\u201d If you\u2019ve never logged in you will have to type \u201cyes\u201d in order to connect Enter the password for the computer you\u2019re trying to access You are now accessing that other computer via the command line (anything you do while connected through this command line window will happen on the other computer) Method 1b: Creating a TAR file on a remote computer and pulling it to your current computer After you followed the steps in 1a, you can create a file and pull it, or just pull an already created file to your local computer Navigate to the folder where the file of interest is stored using a combination of ls (which tells you what is in the current folder and cd \u201cfile_dir\u201d to navigate to the new folder. Once you are in the folder where your file of interest is, type pwd to get the current file path If your file is small and does not need to be TARd skip the next step Creating a TAR Type the following: tar -czvf \u201cfile_name\u201d.tar.gz \u201cyour file path from pwd\u201d . C = create an archive Z = compress a folder with gzip (remove the z and .gz to do a regular tar) V = display progress in terminal (AKA verbose mode, this is also optional) F = (allows you to specify your filename) The \u201c .\u201d will stop the entire absolute file pathway from being there when you extract You should have a new tar file if you ls Transferring Open a new command window, this one will be accessing the computer you\u2019re on now Navigate (using cd and ls) to the folder where you want to put your file (you don\u2019t have to do this but I think it makes things easier) Once you\u2019re there, type the following \u201cscp \u201cyour_username\u201d@\u201dyour_static_ip\u201d:\u201dyour_file_path/filename.tar.gz\u201d . The \u201c .\u201d means put this file where I am right now (that\u2019s why we navigated to the right folder, otherwise you type the filepath of where you want the file to go instead) Change tar.gz to your appropriate file extension Unzipping the file (if needed) gunzip -c foo.tar.gz | tar xopf for tar.gz You don\u2019t need to specify a file path if you are already in the folder where you want things extracted tar -xvf filename.tar -C path you want to copy it to Method 2: From Apple to Apple: Using VPN and Remote Management Make sure your desktop has a static IP (all desktops have them at this time) Change your settings to allow remote management System Preferences -> sharing Check Screen sharing and remote management When you click remote management, select the capabilities you want when logging in remotely Get the VPN software Go to Vpn.arizona.edu NetID: your user name NetID Password: NetID Method: Push (If you\u2019re using Duo Mobile) After logging in click AnyConnect in the left toolbar Download the VPN software. Run the Cisco Anyconnect (you probably have to log in again) When the VPN runs the box may be blank. It should be: vpn.arizona.edu Go to Finder Select the Go menu from the top navigation bar (where the apple is) and go all the way to the bottom of the menu where it says connect to server Type in vnc://\u201dyour_static_IP\u201d and click connect You should now be able to control your desktop","title":"Remote Desktop / SSH / Basic Command line"},{"location":"how_to_notes/data_and_file_transfers/apple_remote_desktop_ssh/#remote-desktop-ssh-basic-command-line","text":"Amanda Triplett 10/2019 Description: Accessing your computer via SSH through your command line with a general example of how to create a TAR file remotely and move it to your home computer. It also outlines how to remotely manage your desktop from mac to mac computer.","title":"Remote Desktop / SSH / Basic Command line"},{"location":"how_to_notes/data_and_file_transfers/apple_remote_desktop_ssh/#method-1a-from-apple-to-apple-ssh","text":"Open a command line and type ssh \u201cyour_username\u201d@\u201dyour_static_IP\u201d If you\u2019ve never logged in you will have to type \u201cyes\u201d in order to connect Enter the password for the computer you\u2019re trying to access You are now accessing that other computer via the command line (anything you do while connected through this command line window will happen on the other computer)","title":"Method 1a: From Apple to Apple: SSH"},{"location":"how_to_notes/data_and_file_transfers/apple_remote_desktop_ssh/#method-1b-creating-a-tar-file-on-a-remote-computer-and-pulling-it-to-your-current-computer","text":"After you followed the steps in 1a, you can create a file and pull it, or just pull an already created file to your local computer Navigate to the folder where the file of interest is stored using a combination of ls (which tells you what is in the current folder and cd \u201cfile_dir\u201d to navigate to the new folder. Once you are in the folder where your file of interest is, type pwd to get the current file path If your file is small and does not need to be TARd skip the next step Creating a TAR Type the following: tar -czvf \u201cfile_name\u201d.tar.gz \u201cyour file path from pwd\u201d . C = create an archive Z = compress a folder with gzip (remove the z and .gz to do a regular tar) V = display progress in terminal (AKA verbose mode, this is also optional) F = (allows you to specify your filename) The \u201c .\u201d will stop the entire absolute file pathway from being there when you extract You should have a new tar file if you ls Transferring Open a new command window, this one will be accessing the computer you\u2019re on now Navigate (using cd and ls) to the folder where you want to put your file (you don\u2019t have to do this but I think it makes things easier) Once you\u2019re there, type the following \u201cscp \u201cyour_username\u201d@\u201dyour_static_ip\u201d:\u201dyour_file_path/filename.tar.gz\u201d . The \u201c .\u201d means put this file where I am right now (that\u2019s why we navigated to the right folder, otherwise you type the filepath of where you want the file to go instead) Change tar.gz to your appropriate file extension Unzipping the file (if needed) gunzip -c foo.tar.gz | tar xopf for tar.gz You don\u2019t need to specify a file path if you are already in the folder where you want things extracted tar -xvf filename.tar -C path you want to copy it to","title":"Method 1b: Creating a TAR file on a remote computer and pulling it to your current computer"},{"location":"how_to_notes/data_and_file_transfers/apple_remote_desktop_ssh/#method-2-from-apple-to-apple-using-vpn-and-remote-management","text":"Make sure your desktop has a static IP (all desktops have them at this time) Change your settings to allow remote management System Preferences -> sharing Check Screen sharing and remote management When you click remote management, select the capabilities you want when logging in remotely Get the VPN software Go to Vpn.arizona.edu NetID: your user name NetID Password: NetID Method: Push (If you\u2019re using Duo Mobile) After logging in click AnyConnect in the left toolbar Download the VPN software. Run the Cisco Anyconnect (you probably have to log in again) When the VPN runs the box may be blank. It should be: vpn.arizona.edu Go to Finder Select the Go menu from the top navigation bar (where the apple is) and go all the way to the bottom of the menu where it says connect to server Type in vnc://\u201dyour_static_IP\u201d and click connect You should now be able to control your desktop","title":"Method 2: From Apple to Apple: Using VPN and Remote Management"},{"location":"how_to_notes/data_and_file_transfers/cyverse/","text":"General Cyverse Information Laura Condon, Jan 2019 Description: Running notes with links and tips for doing file transfers with Cyverse. Feel free to update and add to this document as needed. Creating and Account and Getting Access: In order to upload data, you need to create a Cyverse account and be given permissions by Laura. Provide the username of your account and the e-mail you used to create the account (your UA e-mail) to Laura. Logging in: De.cyverse.org Our Server: You can find under Community Data/avra From command line: /iplant/home/shared/avra GUI file transferring with cyberduck https://cyberduck-quickstart.readthedocs.io/en/latest/# Setting up irods so you can do command line transfers: Sign up for a cyverse account: https://user.cyverse.org/register Install icommands so you can use irods Instructions for setting up on Mac: https://wiki.cyverse.org/wiki/display/DS/Setting+Up+iCommands#SettingUpiCommands-mac For setting up on Powell I used the page above and followed the link for the cent6 To check OS: cat /etc/*release Needed root access to install the irods commands so I logged in as root and then did: sudo yum install https://files.renci.org/pub/irods/releases/4.1.10/centos6/irods-icommands-4.1.10-centos6-x86_64.rpm Then login as your username and setup the cyverse connection according to step 3 Initialize irods by typing iinit and then putting in your user information following part two of the link on step 2 **Command line transfers with irods: ** First initialize irods like this: * _iinit_ * [data.cyverse.org](http://data.cyverse.org/) * Port: 1247 * User: yourusername * Zone: iplant * Then enter your cyverse password To upload: cd to the local directory with the data you want to transfer Then \u2018icd\u2019 to the location on Cyverse where you want to put the data icd /iplant/home/shared/avra #for avra icd /iplant/home/username #for your Cyverse home directory (note this is small and not where you should be storing things) iput -vP \u201cfilename\" To batch upload: iput -K -P -b -r -T folder-where-you-want-files-on-cyverse/ If you icd to where you want to put your files on cyverse then you simply need to put a period after the iput command\u2019 iput -K -P -b -r -T . -K \u201cThis causes iCommands to verify that transferred files weren't corrupted during the transfer by comparing the Checksums computed before and after the transfer.\u201d -P provides progress feedback -b allows for bulk uploads -r includes a folder or directory -T renews socket connection every 10 min If its a tar file you can extract it on Cyverse like this: ibun -x filename.tar new https://wiki.cyverse.org/wiki/display/DS/Dealing+with+Tar+and+other+Archive+Files **** You need to keep it to less than 2GB and several hundred files per Tar or the unzip ibun will fail. ** To Download: Basically the same thing but with iget instead of iput Links with irods info: icommands help: https://wiki.cyverse.org/wiki/display/DS/Using+iCommands Setting up irods commands for R: You should be able to run i commands in R using the system function. e.g. system(\u2018ils') If you get a warning saying command not found even though you have them installed you probably need to modify your R path so it can find them. To check this echo $PATH on command line and compare with system(\u2018echo $PATH) within R (You can also check this with Sys.getevn() to see everything or Sys.getenv(\"PATH\u201d) to just see the path in R) To fix you can set a new path appending the icommands to your current path like this: Sys.setenv(PATH='/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin:/Applications/icommands') Then you also need to add the plugin: Sys.setenv(IRODS_PLUGINS_HOME=\"/Applications/icommands/plugins/\") Then check the path and check that it worked using getenv and testing an icommand. NOTE: If you get an irods host error at this point its probably because you haven\u2019t run iiint PERMANENT Fix: If you don\u2019t want to have to add the Sys.setenv to every script then do the following: vi ~/.Rprofile (just make the file if you don\u2019t have one) Add the following lines: Sys.setenv(PATH='/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin:/Applications/icommands') Sys.setenv(IRODS_PLUGINS_HOME=\"/Applications/icommands/plugins/\") Note this will permanently set your environment variables Running an Analysis on Atmospheres Steps to run on the Cyverse Rstudio App Login: de.cyverse.org Click on Apps - Search for \u2018Rstudio\u2019 Select \u2018rocker-geospatial-3.5.0\u2019 # you can also add it to your favorites Give your analysis a name - you don\u2019t have to change anything though if you don\u2019t want Click \u2018Launch Analysis\u2019 Then if you click on \u2018Analyses\u2019 on the left you should see the job you just launched it will say \u2018running\u2019 but its probably still initializing. Click on the arrow next to its name to open the tab and you will see it launching your application. You will need to login to Cyverse and then R studio the first time it launches.For Rstudio the username and password are both rstudio Once you are in Rstudio - click on the terminal window iinit data.cyverse.org Port: 1247 User: lecondon Zone: iplant Checkout the GitHub repo git clone https://github.com/lecondon/CONUS1_Warming.git \\ Once you have done this you will see \u2018CONUS1_Warming\u2019 in your files tab Go to R scripts and click on the Rmd file you can run from here","title":"General Cyverse Information"},{"location":"how_to_notes/data_and_file_transfers/cyverse/#general-cyverse-information","text":"Laura Condon, Jan 2019","title":"General Cyverse Information"},{"location":"how_to_notes/data_and_file_transfers/cyverse/#description-running-notes-with-links-and-tips-for-doing-file-transfers-with-cyverse-feel-free-to-update-and-add-to-this-document-as-needed","text":"","title":"Description:  Running notes with links and tips for doing file transfers with Cyverse.  Feel free to update and add to this document as needed."},{"location":"how_to_notes/data_and_file_transfers/cyverse/#creating-and-account-and-getting-access","text":"In order to upload data, you need to create a Cyverse account and be given permissions by Laura. Provide the username of your account and the e-mail you used to create the account (your UA e-mail) to Laura. Logging in: De.cyverse.org Our Server: You can find under Community Data/avra From command line: /iplant/home/shared/avra GUI file transferring with cyberduck https://cyberduck-quickstart.readthedocs.io/en/latest/#","title":"Creating and Account and Getting Access:"},{"location":"how_to_notes/data_and_file_transfers/cyverse/#setting-up-irods-so-you-can-do-command-line-transfers","text":"Sign up for a cyverse account: https://user.cyverse.org/register Install icommands so you can use irods Instructions for setting up on Mac: https://wiki.cyverse.org/wiki/display/DS/Setting+Up+iCommands#SettingUpiCommands-mac For setting up on Powell I used the page above and followed the link for the cent6 To check OS: cat /etc/*release Needed root access to install the irods commands so I logged in as root and then did: sudo yum install https://files.renci.org/pub/irods/releases/4.1.10/centos6/irods-icommands-4.1.10-centos6-x86_64.rpm Then login as your username and setup the cyverse connection according to step 3 Initialize irods by typing iinit and then putting in your user information following part two of the link on step 2 **Command line transfers with irods: ** First initialize irods like this: * _iinit_ * [data.cyverse.org](http://data.cyverse.org/) * Port: 1247 * User: yourusername * Zone: iplant * Then enter your cyverse password To upload: cd to the local directory with the data you want to transfer Then \u2018icd\u2019 to the location on Cyverse where you want to put the data icd /iplant/home/shared/avra #for avra icd /iplant/home/username #for your Cyverse home directory (note this is small and not where you should be storing things) iput -vP \u201cfilename\" To batch upload: iput -K -P -b -r -T folder-where-you-want-files-on-cyverse/ If you icd to where you want to put your files on cyverse then you simply need to put a period after the iput command\u2019 iput -K -P -b -r -T . -K \u201cThis causes iCommands to verify that transferred files weren't corrupted during the transfer by comparing the Checksums computed before and after the transfer.\u201d -P provides progress feedback -b allows for bulk uploads -r includes a folder or directory -T renews socket connection every 10 min If its a tar file you can extract it on Cyverse like this: ibun -x filename.tar new https://wiki.cyverse.org/wiki/display/DS/Dealing+with+Tar+and+other+Archive+Files **** You need to keep it to less than 2GB and several hundred files per Tar or the unzip ibun will fail. ** To Download: Basically the same thing but with iget instead of iput Links with irods info: icommands help: https://wiki.cyverse.org/wiki/display/DS/Using+iCommands Setting up irods commands for R: You should be able to run i commands in R using the system function. e.g. system(\u2018ils') If you get a warning saying command not found even though you have them installed you probably need to modify your R path so it can find them. To check this echo $PATH on command line and compare with system(\u2018echo $PATH) within R (You can also check this with Sys.getevn() to see everything or Sys.getenv(\"PATH\u201d) to just see the path in R) To fix you can set a new path appending the icommands to your current path like this: Sys.setenv(PATH='/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin:/Applications/icommands') Then you also need to add the plugin: Sys.setenv(IRODS_PLUGINS_HOME=\"/Applications/icommands/plugins/\") Then check the path and check that it worked using getenv and testing an icommand. NOTE: If you get an irods host error at this point its probably because you haven\u2019t run iiint PERMANENT Fix: If you don\u2019t want to have to add the Sys.setenv to every script then do the following: vi ~/.Rprofile (just make the file if you don\u2019t have one) Add the following lines: Sys.setenv(PATH='/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin:/Applications/icommands') Sys.setenv(IRODS_PLUGINS_HOME=\"/Applications/icommands/plugins/\") Note this will permanently set your environment variables Running an Analysis on Atmospheres Steps to run on the Cyverse Rstudio App Login: de.cyverse.org Click on Apps - Search for \u2018Rstudio\u2019 Select \u2018rocker-geospatial-3.5.0\u2019 # you can also add it to your favorites Give your analysis a name - you don\u2019t have to change anything though if you don\u2019t want Click \u2018Launch Analysis\u2019 Then if you click on \u2018Analyses\u2019 on the left you should see the job you just launched it will say \u2018running\u2019 but its probably still initializing. Click on the arrow next to its name to open the tab and you will see it launching your application. You will need to login to Cyverse and then R studio the first time it launches.For Rstudio the username and password are both rstudio Once you are in Rstudio - click on the terminal window iinit data.cyverse.org Port: 1247 User: lecondon Zone: iplant Checkout the GitHub repo git clone https://github.com/lecondon/CONUS1_Warming.git \\ Once you have done this you will see \u2018CONUS1_Warming\u2019 in your files tab Go to R scripts and click on the Rmd file you can run from here","title":"Setting up irods so you can do command line transfers:"},{"location":"how_to_notes/data_and_file_transfers/globus/","text":"Globus File Transfer Abe, May 8th, 2020 Description Setting up and using globus to transfer files between a local machine and server (uahpc) Software: Globus, UAHPC Datasets: N/A Links: https://www.globus.org/ Globus provides a simple way to transfer files between a local device and a server account instead of using scp or another file transfer method. It is especially useful for large sets of files. Setting up Account: A good reference: https://www.osc.edu/book/export/html/3578 To Login: Visit https://www.globus.org/ , go to login and select \u201cThe University of Arizona\u201d You will be prompted to login with your UA NetID Set-up Endpoints: Endpoints are essentially paths that are linked and configured for streamlined file transfer. Personal endpoints can be setup for personal systems: research desktop or personal laptop When logged-in, click on \u201cendpoints\u201d on the left panel At the top select \u201ccreate personal endpoint\u201d Download Globus Connect Personal (follow the install instructions) You can select the default home directory or specify a different directory for the endpoint if you wish You will also name the endpoint (ex: \u201cAbe\u2019s Research Desktop\u201d) You should now have a personal endpoint for the device which you are currently working on Transfering Files To transfer files between this personal endpoint and the UAHPC server, you will have to search for it on the globus website: When logged-in, click on \u201cfile manager\u201d on the left panel The left panel is where you want to transfer from...so to transfer something from your desktop click in the \u201ccollections\u201d search bar and select the personal endpoint that you just created (for ex: \u201cAbe\u2019s Research Desktop\u201d) From here you can navigate the system for the directory or files that you wish to transfer On the right window panel is where you will select the server endpoint (or another personal endpoint). For UAHPC you search UA HPC Filesystems More info found here: https://public.confluence.arizona.edu/display/UAHPC/Transferring+Files Once in the UA server you can navigate your UAHPC directories and go to where you want to transfer the files. (I had to physically type in the directory I wanted on the HPC) Click \u201cStart\u201d once the correct file and directories are selected. Note you can transfer to/from either endpoint You\u2019ll get a confirmation email once it was successfully transferred Other links and resources: Install guide of Mac: https://docs.globus.org/how-to/globus-connect-personal-mac/ Penn State Explanation: https://www.youtube.com/watch?v=iIfeVxplZ8U Extra Information for NCAR HPC (Cheyenne): This link has helpful resources: https://www2.cisl.ucar.edu/resources/storage-and-file-systems/globus-file-transfers Log in to your Globus account Go to File manager Go to panel view so you can see the source and destination of your files Type NCAR GLADE into the collection for either source or destination, select your personal endpoint for the other You will be asked to log in, use your normal log in credentials when you are accessing NCAR resources through terminal Click advanced and for certificate length type \u2018720\u2019 this will allow you to extend the credential to 30 days as opposed to the usual 24 You will authenticate your log in whatever way you have it set up, likely Duo Go to /glade/scratch/\u201dyour_username\u201d and you should see the file tree as you have it set up Start transfer! Information for Google Drive transfers: Follow the steps in the link below to setup your Google Drive account as an endpoint. https://public.confluence.arizona.edu/display/UAHPC/Transferring+Files#TransferringFiles-GoogleDrive","title":"Globus File Transfer"},{"location":"how_to_notes/data_and_file_transfers/globus/#globus-file-transfer","text":"Abe, May 8th, 2020","title":"Globus File Transfer"},{"location":"how_to_notes/data_and_file_transfers/globus/#description","text":"Setting up and using globus to transfer files between a local machine and server (uahpc) Software: Globus, UAHPC Datasets: N/A Links: https://www.globus.org/ Globus provides a simple way to transfer files between a local device and a server account instead of using scp or another file transfer method. It is especially useful for large sets of files.","title":"Description"},{"location":"how_to_notes/data_and_file_transfers/globus/#setting-up-account","text":"A good reference: https://www.osc.edu/book/export/html/3578 To Login: Visit https://www.globus.org/ , go to login and select \u201cThe University of Arizona\u201d You will be prompted to login with your UA NetID","title":"Setting up Account:"},{"location":"how_to_notes/data_and_file_transfers/globus/#set-up-endpoints","text":"Endpoints are essentially paths that are linked and configured for streamlined file transfer. Personal endpoints can be setup for personal systems: research desktop or personal laptop When logged-in, click on \u201cendpoints\u201d on the left panel At the top select \u201ccreate personal endpoint\u201d Download Globus Connect Personal (follow the install instructions) You can select the default home directory or specify a different directory for the endpoint if you wish You will also name the endpoint (ex: \u201cAbe\u2019s Research Desktop\u201d) You should now have a personal endpoint for the device which you are currently working on","title":"Set-up Endpoints:"},{"location":"how_to_notes/data_and_file_transfers/globus/#transfering-files","text":"To transfer files between this personal endpoint and the UAHPC server, you will have to search for it on the globus website: When logged-in, click on \u201cfile manager\u201d on the left panel The left panel is where you want to transfer from...so to transfer something from your desktop click in the \u201ccollections\u201d search bar and select the personal endpoint that you just created (for ex: \u201cAbe\u2019s Research Desktop\u201d) From here you can navigate the system for the directory or files that you wish to transfer On the right window panel is where you will select the server endpoint (or another personal endpoint). For UAHPC you search UA HPC Filesystems More info found here: https://public.confluence.arizona.edu/display/UAHPC/Transferring+Files Once in the UA server you can navigate your UAHPC directories and go to where you want to transfer the files. (I had to physically type in the directory I wanted on the HPC) Click \u201cStart\u201d once the correct file and directories are selected. Note you can transfer to/from either endpoint You\u2019ll get a confirmation email once it was successfully transferred Other links and resources: Install guide of Mac: https://docs.globus.org/how-to/globus-connect-personal-mac/ Penn State Explanation: https://www.youtube.com/watch?v=iIfeVxplZ8U","title":"Transfering Files"},{"location":"how_to_notes/data_and_file_transfers/globus/#extra-information-for-ncar-hpc-cheyenne","text":"This link has helpful resources: https://www2.cisl.ucar.edu/resources/storage-and-file-systems/globus-file-transfers Log in to your Globus account Go to File manager Go to panel view so you can see the source and destination of your files Type NCAR GLADE into the collection for either source or destination, select your personal endpoint for the other You will be asked to log in, use your normal log in credentials when you are accessing NCAR resources through terminal Click advanced and for certificate length type \u2018720\u2019 this will allow you to extend the credential to 30 days as opposed to the usual 24 You will authenticate your log in whatever way you have it set up, likely Duo Go to /glade/scratch/\u201dyour_username\u201d and you should see the file tree as you have it set up Start transfer!","title":"Extra Information for NCAR HPC (Cheyenne):"},{"location":"how_to_notes/data_and_file_transfers/globus/#information-for-google-drive-transfers","text":"Follow the steps in the link below to setup your Google Drive account as an endpoint. https://public.confluence.arizona.edu/display/UAHPC/Transferring+Files#TransferringFiles-GoogleDrive","title":"Information for Google Drive transfers:"},{"location":"useful_external_resources/great_python_packages/","text":"","title":"Great python packages"},{"location":"useful_external_resources/parflow_resources/","text":"","title":"Parflow resources"},{"location":"useful_external_resources/research_repos/","text":"","title":"Research repos"}]}