<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><link rel="canonical" href="https://condon-lab.github.io/Condon_Lab_Docs/how_to_notes/HPC_systems/UA_HPC/" />
      <link rel="shortcut icon" href="../../../img/favicon.ico" />
    <title>UA HPC Information - Condon Research Group Docs</title>
    <link rel="stylesheet" href="../../../css/theme.css" />
    <link rel="stylesheet" href="../../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "UA HPC Information";
        var mkdocs_page_input_path = "how_to_notes/HPC_systems/UA_HPC.md";
        var mkdocs_page_url = "/Condon_Lab_Docs/how_to_notes/HPC_systems/UA_HPC/";
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../../.." class="icon icon-home"> Condon Research Group Docs
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../..">Condon Research Group Docs</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Admin</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../../admin/pcard/">Pcard</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../admin/travel_autorization/">Information Regarding Travel Authorization</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../admin/travel_reimbursement/">Information Regarding Reimbursement for Travel</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Example scripts</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../../example_scripts/">Example scripts</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Group meetings</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../../group_meetings/">Group Meetings</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">How to notes</span></p>
              <ul class="current">
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">HPC systems</a>
    <ul class="current">
                <li class="toctree-l2 current"><a class="reference internal current" href="./">UA HPC Information</a>
    <ul class="current">
    <li class="toctree-l3"><a class="reference internal" href="#description">Description</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#ocelote-quick-start">Ocelote Quick Start</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#puma-quick-start">Puma Quick Start:</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#uits-account-management">UITS Account Management</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#hpc-portal">HPC portal</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#logging-in">Logging in</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#hpc-browser-dashboard-ondemand">HPC Browser Dashboard (OnDemand)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#making-a-run-script">Making a run script</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#link-for-computing-account-management">Link for computing account management</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#instructions-for-setting-up-and-account">Instructions for setting up and account</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#accessing-software-via-module-commands">Accessing Software via Module commands</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#storage-and-limits">Storage and limits</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#uploading-files-to-uahpc">Uploading files to UAHPC:</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#monitoring-jobs">Monitoring jobs</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#getting-started">Getting started</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#parflow-on-uahpc">ParFlow on UAHPC</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#example-pbs-script">Example pbs script</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#slurm-scripts">SLURM scripts</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#debug-queue">Debug queue</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#time-allocation">Time Allocation</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#accessing-and-using-software-like-python">Accessing and using software (like Python)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#python">Python:</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#virtual-environments">Virtual Environments</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#conda">Conda</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#how-to-do-a-basic-script-in-python">How to do a basic script in Python</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#how-to-run-a-more-complicated-python-script-in-terminal-on-uahpc">How to run a more complicated Python script in terminal on UAHPC</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#how-to-run-python-in-a-jupiter-notebook-on-uahpc">How to run Python in a Jupiter notebook on UAHPC</a>
    </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../cheyenne/">Running on Cheyenne (NCAR)</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../cori/">Cori</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../parflow_install/">ParFlow Install</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../verde/">Verde</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">Data and file transfers</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../data_and_file_transfers/apple_remote_desktop_ssh/">Remote Desktop / SSH / Basic Command line</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../data_and_file_transfers/cyverse/">General Cyverse Information</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../data_and_file_transfers/globus/">Globus File Transfer</a>
                </li>
    </ul>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Useful external resources</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../../useful_external_resources/great_python_packages/">Great python packages</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../useful_external_resources/parflow_resources/">Parflow resources</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../useful_external_resources/research_repos/">Research repos</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../..">Condon Research Group Docs</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../.." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">How to notes</li>
          <li class="breadcrumb-item">HPC systems</li>
      <li class="breadcrumb-item active">UA HPC Information</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="ua-hpc-information">UA HPC Information</h1>
<p><strong><em>Laura Condon, Oct, 2018 (Updated) Quinn Hull, Nov 2020</em></strong></p>
<p><strong><em>(Updated) Rachel Spinti, Feb 2021</em></strong></p>
<h2 id="description">Description</h2>
<p>Running notes with links and tips for running on the UA HPC system.  Feel free to update and add to this document as needed. </p>
<h2 id="ocelote-quick-start">Ocelote Quick Start</h2>
<p>A good starting point. Tutorial highlights essential steps in running a basic job on HPC Ocelote.</p>
<ol>
<li>How to log in</li>
<li>What a login node is</li>
<li>What a job scheduler is </li>
<li>How to access software</li>
<li>How to run a job on HPC</li>
</ol>
<p><a href="https://public.confluence.arizona.edu/display/UAHPC/Ocelote+Quick+Start">https://public.confluence.arizona.edu/display/UAHPC/Ocelote+Quick+Start</a> </p>
<h2 id="puma-quick-start">Puma Quick Start:</h2>
<p>Tutorial highlights essential steps in running a basic job on HPC Puma.</p>
<ol>
<li>How to log in</li>
<li>What a login node is</li>
<li>How to access software (Note: it is different than Ocelote, see <a href="https://public.confluence.arizona.edu/display/UAHPC/Puma+Quick+Start#PumaQuickStart-AccessingSoftware">https://public.confluence.arizona.edu/display/UAHPC/Puma+Quick+Start#PumaQuickStart-AccessingSoftware</a>)</li>
<li>How to write a SLURM script</li>
<li>How to run a job on Puma</li>
</ol>
<p><a href="https://public.confluence.arizona.edu/display/UAHPC/Puma+Quick+Start">https://public.confluence.arizona.edu/display/UAHPC/Puma+Quick+Start</a></p>
<h2 id="uits-account-management">UITS Account Management</h2>
<p>To set-up a UA HPC account if you have not activated one. Sign-in and go to “Manage Your Accounts”</p>
<p><a href="https://account.arizona.edu/welcome">https://account.arizona.edu/welcome</a></p>
<h2 id="hpc-portal">HPC portal</h2>
<p><a href="https://portal.hpc.arizona.edu/portal/?new0=lecondon">https://portal.hpc.arizona.edu</a></p>
<h2 id="logging-in">Logging in</h2>
<p><code>_ssh username@hpc.arizona.edu</code> (e.g. roberthull@hpc.arizona.edu)_</p>
<p>Should give you a choice to connect to Ocelote - if not you can type <code>menuon</code> to get the menu</p>
<ul>
<li>Type <code>ocelote</code> to enter the real HPC system</li>
</ul>
<h2 id="hpc-browser-dashboard-ondemand">HPC Browser Dashboard (OnDemand)</h2>
<p><a href="https://ood.hpc.arizona.edu/pun/sys/dashboard">https://ood.hpc.arizona.edu/pun/sys/dashboard</a></p>
<p>A GUI that allows you to monitor jobs and access files outside the terminal. Can be easier to view and/or edit files this way than in command line.</p>
<h2 id="making-a-run-script">Making a run script</h2>
<p>To execute a ‘job’ (ex: execute tclsh script) on HPC, one must ‘submit’ a request using the scheduling software PBS. </p>
<p><a href="https://public.confluence.arizona.edu/pages/viewpage.action?pageId=86409309">https://public.confluence.arizona.edu/pages/viewpage.action?pageId=86409309</a></p>
<ul>
<li>PBS Overview: Jobs are submitted to the batch system using PBS scripts that specify the job's required resources such as number of nodes, cpus, memory, group, wallclock time. </li>
</ul>
<p><a href="https://jobbuilder.hpc.arizona.edu//">https://jobbuilder.hpc.arizona.edu//</a></p>
<ul>
<li>This site is helpful to start, but is a bit outdated: see the example pbs script at the end of this doc for more detail</li>
</ul>
<p><a href="https://public.confluence.arizona.edu/display/UAHPC/Ocelote+Quick+Start">https://public.confluence.arizona.edu/display/UAHPC/Ocelote+Quick+Start</a> </p>
<p>A (very helpful) “hello world” tutorial including job submission via pbs script</p>
<h2 id="link-for-computing-account-management">Link for computing account management</h2>
<p><strong><a href="https://account.arizona.edu/welcome">https://account.arizona.edu/welcome</a></strong></p>
<h2 id="instructions-for-setting-up-and-account">Instructions for setting up and account</h2>
<p><a href="https://public.confluence.arizona.edu/display/UAHPC/Account+Creation">https://public.confluence.arizona.edu/display/UAHPC/Account+Creation</a> </p>
<h2 id="accessing-software-via-module-commands">Accessing Software via Module commands</h2>
<p><strong><a href="https://docs.hpc.arizona.edu/display/UAHPC/Accessing+Software">https://docs.hpc.arizona.edu/display/UAHPC/Accessing+Software</a></strong></p>
<p><code>Module list</code>  (gives loaded modules)</p>
<p><code>Module avail</code> (gives list of all modules)</p>
<p><code>Module show NAME</code> (gives info on a specific module </p>
<h2 id="storage-and-limits">Storage and limits</h2>
<p><a href="https://docs.hpc.arizona.edu/display/UAHPC/Allocation+and+Limits">https://docs.hpc.arizona.edu/display/UAHPC/Allocation+and+Limits</a></p>
<p>50GB on <code>home/uxx/netid</code> (ex home/u8/roberthull)</p>
<p>200GB (no Backups) on extra/netid</p>
<p><code>/temp</code> - 840GB available per node. You can use this during the job and then do a final write to a shared array. (You cannot permanently save anything to this location)</p>
<p>No more than 600files/GB ~1.6 MB per file</p>
<p><img alt="tabel describing UA HPC storage allocation" src="../../../images/ua_hpc_storage.png" /></p>
<p>There is a total time limitation for our group (36,000 hours/month), use command <em>va</em> to check the remaining time of the month.</p>
<h2 id="uploading-files-to-uahpc">Uploading files to UAHPC:</h2>
<p><a href="https://public.confluence.arizona.edu/display/UAHPC/Transferring+Files">https://public.confluence.arizona.edu/display/UAHPC/Transferring+Files</a> </p>
<ol>
<li>
<p>(small files) Use the GUI web terminal <a href="https://ood.hpc.arizona.edu/pun/sys/dashboard">https://ood.hpc.arizona.edu/pun/sys/dashboard</a></p>
<ol>
<li><a href="https://ood.hpc.arizona.edu/pun/sys/dashboard">Log on</a></li>
<li><a href="https://ood.hpc.arizona.edu/pun/sys/dashboard">Click ‘Files’ in top left corner </a></li>
<li>Select Directory to which you want to add files (e.g. </li>
<li>Click <code>Upload</code> and navigate to the local file location.</li>
<li>To use the file, navigate to the file location by clicking<code>Open in terminal</code></li>
</ol>
</li>
<li>
<p>(big files &lt;100 GB). Use sftp, scp or rsync using filexfer.hpc.arizona.edu</p>
<p>Copy your files through through the transfer node like below: </p>
<pre><code>_scp -rp file.txt &amp;lt;netid&gt;[@filexfer.hpc.arizona.edu](mailto:lecondon@filexfer.hpc.arizona.edu):&amp;lt;directory&gt;_
</code></pre>
<p>Example: If I (roberthull) want to upload a python script (test.py) to my home directory (/home/u8/roberthull) the command would look like the below</p>
<pre><code>_scp -rp test.py roberthull[@filexfer.hpc.arizona.edu](mailto:lecondon@filexfer.hpc.arizona.edu):_/home/u8/roberthull
</code></pre>
<p>*** <strong>Remember that on all HPC systems your files will be purged after a certain amount of time it is your job to be aware of the purge policies and copy your outputs somewhere appropriate for longer term storage.</strong></p>
<p>*** <strong>Note as shown above scp requires <a href="http://filexfer.hpc.arizona.edu/">http://filexfer.hpc.arizona.edu/</a> rather than <a href="http://hpc.arizona.edu/">hpc.arizona.edu</a></strong></p>
</li>
</ol>
<h2 id="monitoring-jobs">Monitoring jobs</h2>
<p><strong>[https://public.confluence.arizona.edu/pages/viewpage.action?pageId=86409309](https://public.confluence.arizona.edu/pages/viewpage.action?pageId=86409309</strong></p>
<p>*Note <code>qstat -u</code>  will truncate the job_id to get it not to do this you need to use</p>
<p><code>qstat -antsw1u lecondon</code></p>
<p><img alt="list of qstat commands" src="../../../images/qstat_commands.png" /></p>
<h2 id="getting-started">Getting started</h2>
<p>Steps for getting started and running your first job:</p>
<ol>
<li>Get an account see instructions above</li>
<li>Login <code>ssh username@hpc.arizona.edu</code> - _remember when you first login you need to type the correct number to actually get into Ocelote </li>
<li>Build ParFlow on Ocelote (follow the instructions below)</li>
<li>
<p>Upload your files to the directory you want to run in. Look at the storage limits links above to learn about the places you can store files. Your home directory is VERY small 50GB so you don’t want to keep to much here but it’s a good place to put things that you don’t want to be deleted. The ‘extra’ directory has more space and is a better place to run. You can copy your files using the scp command through the filexfer node like this:</p>
<p><code>scp -rp file.txt [lecondon@filexfer.hpc.arizona.edu](mailto:lecondon@filexfer.hpc.arizona.edu):/extra/lecondon/wash_base</code></p>
<p>*** **Remember that on all HPC systems your files will be purged after a certain amount of time it is your job to be aware of the purge policies and copy your outputs somewhere appropriate for longer term storage. **</p>
<p>*** <strong>Note as shown above scp requires <a href="http://filexfer.hpc.arizona.edu/">http://filexfer.hpc.arizona.edu/</a> rather than <a href="http://hpc.arizona.edu/">hpc.arizona.edu</a></strong></p>
</li>
<li>
<p>Build a run script. You can’t just tclsh on a HPC machine you need to ‘submit’ your job to request time and specify how much resources you will need. See the example run scripts below and the link above for generating a run script.  </p>
<p>Note: There are different queues that you can submit to. If you just want to test something quickly the debug queue is a good option. You can read about the different queues in the storage and limits section and see an example of a debug submission below</p>
</li>
<li>
<p>Submit your job to the queue by running the qsub command on the run script you just created. </p>
<pre><code>qsub myrunscrp.pbs
</code></pre>
</li>
<li>
<p>Minitor your job. You can see when your job starts running and finishes using the commands in the run monitoring section. </p>
</li>
<li>Copy your outputs back to your local machine using scp commands like in step four or upload to cyverse using irods commands.  </li>
</ol>
<h2 id="parflow-on-uahpc">ParFlow on UAHPC</h2>
<p><strong>Running with an existing version of ParFlow</strong></p>
<p>_If you want to run with one of the versions of ParFlow that has already been build just add these lines to your bashrc and make sure that you aren’t also setting ‘PARFLOW_DIR’ in in your bashrc file. _</p>
<pre><code class="language-bash">ssh [username@hpc.arizona.edu](mailto:username@hpc.arizona.edu)

cd $HOME

vi ~/.bashrc

# Add the following lines to this file

module load unsupported

module load lecondon/parflow/latest

# You can check if things look right using the commands

which parflow

echo $PARFLOW_DIR
</code></pre>
<p><strong>Adding a new ParFlow Module</strong></p>
<p>Refer to the README file in:  <code>/unsupported/lecondon/parflow/</code></p>
<p><strong>These are the old instructions for building ParFlow with Autoconf. Ignore these and use the CMAKE ones</strong></p>
<p>Download ParFlow</p>
<p><code>git clone  https://github.com/parflow/parflow</code></p>
<p>Rename this directory to whatever you want</p>
<p>Setup environment</p>
<p><code>vi ~/.bashrc</code></p>
<p>Add the following lines to the user specified portion:</p>
<pre><code>module load gcc

module load hypre

module load silo

export PARFLOW_DIR=/home/PATH_TO_YOUR_PARFLOW_DIR

PATH=$PATH:$PARFLOW_DIR/bin
</code></pre>
<p><code>source ~/.bashrc</code></p>
<p>Build ParFlow</p>
<pre><code class="language-bash">cd pfsimulator

./configure  --prefix=$PARFLOW_DIR --with-amps=mpi1 --with-clm --with-hypre=$HYPRE_BASE --with-silo=$SILO_BASE --with-amps-sequential-io

make -j 14_

_make install_

_Build PFtools_

_cd ../pftools_

_./configure  --prefix=$PARFLOW_DIR --with-amps=mpi1 --with-clm --with-hypre=$HYPRE_BASE --with-silo=$SILO_BASE --with-amps-sequential-io_

make -j 14

make install
</code></pre>
<p>Run tests</p>
<pre><code class="language-bash">
cd ../test

make check
</code></pre>
<h2 id="example-pbs-script">Example pbs script</h2>
<p>This is an example of what a job submit script should look like. You can generate one based off of this or using the link above</p>
<pre><code class="language-bash"># Your job will use 1 node, 8 cores, and 48gb of memory total.

#PBS -q standard

#PBS -l select=1:ncpus=16:mem=48gb:pcmem=6gb

### Specify a name for the job

#PBS -N test

### Specify the group name

#PBS -W group_list=lecondon

### Walltime is how long your job will run

#PBS -l walltime=00:50:00

### Joins standard error and standard out

#PBS -j oe


cd /home/u18/lecondon/Test/washita/tcl_scripts

tclsh Dist_Forcings.tcl

tclsh LW_Test.tcl
</code></pre>
<p><strong><em>Note: the “place=pack:shared” and “cput” lines in the HPC job builder guide website are no longer necessary as of July 2019. </em></strong></p>
<h2 id="slurm-scripts">SLURM scripts</h2>
<p>Puma uses SLURM to submit jobs instead of PBS. The link above has a PBS to SLURM Rosetta stone, so you can convert your PBS script. Otherwise, the Puma Quick start guide contains a sample SLURM script (<a href="https://public.confluence.arizona.edu/display/UAHPC/Puma+Quick+Start#PumaQuickStart-WritingaSLURMSubmissionScript">https://public.confluence.arizona.edu/display/UAHPC/Puma+Quick+Start#PumaQuickStart-WritingaSLURMSubmissionScript)</a>).</p>
<h2 id="debug-queue">Debug queue</h2>
<p>The debug queue is a high-priority queue with a short runtime that is used to debug your runs before sending them to the standard or other queue. More information can be found here: </p>
<p><a href="https://docs.hpc.arizona.edu/display/UAHPC/Allocation+and+Limits">https://docs.hpc.arizona.edu/display/UAHPC/Allocation+and+Limits</a></p>
<p><strong>Example pbs script for debug queue (comments are in <code>{}</code> and not to be included in script):</strong></p>
<pre><code class="language-bash"># Your job will use 1 node, 8 cores, and 48gb of memory total.

#PBS -q debug       {changing specified queue to the debug queue}

#PBS -l select=1:ncpus=16:mem=48gb:pcmem=6gb    

{cannot exceed 2 nodes or 56 total cores}

### Specify a name for the job

#PBS -N test

### Specify the group name

#PBS -W group_list=lecondon

### Walltime is how long your job will run

#PBS -l walltime=00:10:00       {walltime cannot exceed 10 minutes}

### Joins standard error and standard out

#PBS -j oe
</code></pre>
<p>Once the job has been submitted to the debug queue and been executed, the error and output messages for the job will indicate whether the job ran for the total allotted time or ran into an error before then.</p>
<h2 id="time-allocation">Time Allocation</h2>
<p>Each PI has a finite amount of time allocation each month. After each run completes, the _run_name.o####### _file will show the amount of time left in the standard queue each month. Each time you submit a script, the system makes sure that your request can be fulfilled in the given queue - otherwise, it returns an error message. If you’re requesting more time than you have for the rest of the month, you can: 1) reduce the amount of time you are requesting, 2) submit it to the windfall queue, or 3) wait until the end of the month and try again next month. Your advisor may not be okay with you trying option 3.</p>
<h2 id="accessing-and-using-software-like-python">Accessing and using software (like Python)</h2>
<p>General: <a href="https://public.confluence.arizona.edu/display/UAHPC/Accessing+Software">https://public.confluence.arizona.edu/display/UAHPC/Accessing+Software</a> </p>
<p>Python: <a href="https://public.confluence.arizona.edu/display/UAHPC/Using+and+Installing+Python">https://public.confluence.arizona.edu/display/UAHPC/Using+and+Installing+Python</a> </p>
<p>Conda: <a href="https://public.confluence.arizona.edu/display/UAHPC/Open+On+Demand">https://public.confluence.arizona.edu/display/UAHPC/Open+On+Demand</a> </p>
<p><strong>Summary: </strong>Many software are installed on the server (&gt; 100). Note that both Python and Anaconda exist, which allow for the installation of Python packages and usage of conda virtual environments. </p>
<h2 id="python">Python:</h2>
<p>There are four versions of Python on Ocelote, (up to Python 3.6.5). </p>
<pre><code>Python 3.6.5 is loaded with ‘module load python’, and includes machine learning packages like Tensorflow. (Does it include PyTorch?)
</code></pre>
<p><img alt="Python versions available in Ocelot and their included pacakges" src="../../../images/ocelot_py_versions.png" /></p>
<p>There are three versions of Python in ElGato (up to Python 3.8.0).</p>
<pre><code>Python 3.8.0 is loaded with ‘module load python/3.8/3.8.0’
</code></pre>
<p><img alt="Python versions available in El Gato and their included pacakges" src="../../../images/el_gato_py_versions.png" /></p>
<p>There are two versions of Python in Puma (up to Python 3.8.2)</p>
<ul>
<li>Note that for DL software, Puma is best</li>
<li><a href="https://public.confluence.arizona.edu/display/UAHPC/Puma+Quick+Start">https://public.confluence.arizona.edu/display/UAHPC/Puma+Quick+Start</a> </li>
</ul>
<h2 id="virtual-environments">Virtual Environments</h2>
<pre><code>It is useful to set up a virtualenv within your working directory.
</code></pre>
<p><img alt="Virtual Envireonment instructions" src="../../../images/ua_hpc_virtual_env_instructions.png" /></p>
<h2 id="conda">Conda</h2>
<pre><code>Anaconda is available as a module, with its requisite packages and access to virtual environments (too?)
</code></pre>
<p><img alt="anaconda instructions" src="../../../images/anaconda_insturctions.png" /></p>
<p>Look at Conda Environments: <code>conda env list</code></p>
<p>Create Conda Environments: <code>conda create --name -myenv</code></p>
<p>Activate Conda Environments: <code>conda activate</code> and <code>conda deactivate</code></p>
<h2 id="how-to-do-a-basic-script-in-python">How to do a basic script in Python</h2>
<ol>
<li>Log in via terminal or bash<ul>
<li>see the "Logging In" section</li>
</ul>
</li>
<li>Find Python modules<ul>
<li><code>module avail python</code></li>
<li>This will list availalbe python modules. e.g. <code>python/3.6/3.65</code></li>
</ul>
</li>
<li>Load Python<ul>
<li><code>module load python/3.6/3.65</code></li>
</ul>
</li>
<li>Add/Create/Locate the script you want to run
    <code>pytorch_helloworld.py</code> -&gt; <code>print("hello world")</code></li>
<li>Create and edit pbs script (note important fields)<pre><code>touch pytorch_helloworld.pbs
vim pytorch_helloworld.pbs
</code></pre>
</li>
</ol>
<pre><code># --------------------------------------------------------------
### PART 1: Requests resources to run your job.
# --------------------------------------------------------------
### Optional. Set the job name
#PBS -N pytorch_helloworld
### REQUIRED. Specify the PI group for this job
#PBS -W group_list=lecondon
### Optional. Request email when job begins and ends
### PBS -m bea
### Optional. Specify email address to use for notification
### PBS -M roberthull@email.arizona.edu
### REQUIRED. Set the queue for your job.
#PBS -q windfall
### REQUIRED. Set the number of nodes, cores and memory that will be used for this job
### pcmem is optional as it defaults to 6gb per core. Only required for high memory = 42gb.
#PBS -l select=1:ncpus=1:mem=6gb:pcmem=6gb
### REQUIRED. Specify &quot;wallclock time&quot; required for this job, hhh:mm:ss
#PBS -l walltime=0:1:0
### Optional. cput = time x ncpus. Default: cput = walltime x ncpus.
#PBS -l cput=0:1:0
# --------------------------------------------------------------
### PART 2: Executes bash commands to run your job
# --------------------------------------------------------------
### Load required modules/libraries if needed
module load python/3.6/3.6.5
### change to your scripts directory
cd ~/python
### run your work
python3 pytorch_helloworld.py
sleep 10i

</code></pre>
<ol>
<li>
<p>Submit job via qsub
    <code>qsub pytorch_helloworld.pbs3592178.head1.cm.cluster</code></p>
</li>
<li>
<p>Follow job progression via qstat</p>
</li>
</ol>
<p><img alt="terminal output for qstat progress" src="../../../images/qstat_progress.png" /></p>
<ol>
<li>View results of job  <code>&lt;jobname&gt;.o&lt;number&gt;</code> for results, `<jobname>.e<number> for errors<ul>
<li>Look at results: <code>vim pytorch_helloworld.o3692196</code> -&gt; <code>hello world</code></li>
<li>If there are errors use: <code>vim pytorch_helloworld.e3692196</code></li>
</ul>
</li>
</ol>
<h2 id="how-to-run-a-more-complicated-python-script-in-terminal-on-uahpc">How to run a more complicated Python script in terminal on UAHPC</h2>
<ol>
<li>Log in via terminal or bash (go to Ocelote)<ul>
<li>See the "Logging in" section</li>
</ul>
</li>
<li>Log in via terminal or bash<ul>
<li>see the "Logging In" section</li>
</ul>
</li>
<li>Find Python modules<ul>
<li><code>module avail python</code></li>
<li>This will list availalbe python modules. e.g. <code>python/3.6/3.65</code></li>
</ul>
</li>
<li>Load Python<ul>
<li><code>module load python/3.6/3.65</code></li>
<li>Take care that you load the version of python that you need. That includes considerations of what packages are available via UAHPC, and how you’ve installed custom packages in your virtual environment</li>
<li>Also, you can check what python versions are available and/or are loaded using <code>module list</code></li>
</ul>
</li>
<li>Activate your virtual environment (create it if necessary)<ul>
<li><code>virtualenv --system-site-packages ~/mypyenv_oc_py.3.6</code></li>
<li><code>source ~/mypyenv_oc_py3.6/bin/activate</code></li>
<li>Take care to use a virtual environment compatible with your version of python and with your super computer of choice (Ocelote v ElGato)</li>
</ul>
</li>
<li>
<p>Add/Create/Locate the script you want to run</p>
<ul>
<li><code>Pytorch_gpus.py</code>  →   runs a simple gpu script</li>
<li>Note if you use custom packages you will need to manually reference the location of your virtual environment in the following way within your python script:</li>
</ul>
</li>
<li>
<p>Check which packages you have in your virtual environment, <code>pip list</code> (note that python/3.6/3.6.5 for ocelote has almost everything you would ever want to have…)</p>
</li>
<li>Install any packages that you think you need to run your desired script<ul>
<li><code>pip install &lt;name_of_package&gt;</code></li>
</ul>
</li>
<li>Create and edit pbs script (note important fields)</li>
</ol>
<pre><code class="language-bash">#!/bin/bash 

# --------------------------------------------------------------

### PART 1: Requests resources to run your job.

# --------------------------------------------------------------

### Optinal. Set the job name

#PBS -N pytorch_gpus_n1

### REQUIRED. Specify the PI group for this job

#PBS -W group_list=lecondon

### Optional. Request email when job begins and ends

### PBS -m bea

### Optional. Specify email address to use for notification

### PBS -M &amp;lt;YOUR NETID&gt;@email.arizona.edu

### REQUIRED. Set the queue for your job.

#PBS -q windfall

### REQUIRED. Set the number of nodes, cores and memory that will be used for this job

### pcmem is optional as it defaults to 6gb per core. Only required for high memory = 42gb.

#PBS -l select=1:ncpus=1:mem=6gb

### REQUIRED. Specify &quot;wallclock time&quot; required for this job, hhh:mm:ss

#PBS -l walltime=0:1:0

### Optional. cput = time x ncpus. Default: cput = walltime x ncpus.

#PBS -l cput=0:1:0

# --------------------------------------------------------------

### PART 2: Executes bash commands to run your job

# --------------------------------------------------------------

### Load required modules/libraries if needed

module load python/3.6/3.6.5

#### Activate virtual environment

source ~/mypyenv_oc_py3.6/bin/activate

### change to your script’s directory

cd ~/python

### Run your work

python3 pytorch_gpus.py

sleep 10
</code></pre>
<ul>
<li>A quick note about Requesting Resources and Node Types (Standard, GPU, or High Memory) via the select statement. The basic `select statement is:<pre><code>#PBS -l select=x:ncpus=Y:mem=Zgb

Where:
 x = The number of nodes or units of the resources required
 Y = The number of cores (individual processors) required on each node
 Z = The amount of memory (in mb or gb) required on each node
</code></pre>
</li>
</ul>
<p>‘Normal’ Requests</p>
<pre><code>    Ocelote
        For Ocelote all of the standard nodes have 28 cores and 6GB per core, pcmem=6gb can be added to the line or left off, and it will default to 6gm. The following select statement would request one complete node:

         #PBS -l select=28:ncpus=28:pcmem=6gb

        This is an example of a job that uses two nodes

        #PBS -l select=2:ncpus=28:mem=16gb

    El Gato
        On El Gato all of the standard nodes of 16 cores and 4GB per core. The maximum available memory on a standard El Gato node is 62GB, leaving the differences for the operating system. The following select statement would request one complete node

         #PBS -l select=1:ncpus=16:pcmem=4gb
</code></pre>
<p>‘GPU’ Requests</p>
<pre><code>    Ocelote
        On Ocelote there are 46 Nvidia P100 nodes available with 28 cores and 224 GB of memory. Users of these nodes can use either their standard allocation of cpu hours or windfall time. Jobs that do not need GPU's will not run on them, including windfall jobs taht do no need the GPU. EAch job will have exclusive access to the node to prevent contention between the CPU cores and the GPU. Each group is limited to ten GPU nodes concurrently.

        This requests a CentOs 7 GPU node:
            #PBS -l select=1:ncpus=28:mem=224gb:np100s=1:os7=True

            or

            #PBS -l select=1:ncpus=28:mem=224gb:ngpus=1:os7=True

        This requests a CentOS 6 GPU node:

            #PBS -l select=1:ncpus=28:mem=224gb:np100s=1

        Ocelote has a single node with two GPUs that may be requested with

            #PBS -l select=1:ncpus=28:mem=224gb:np100s=2

    El Gato
        Unlike Ocelote, El Gato has cgroups enabled which allows for selecting a partial GPU node. Memory requests should be scaled by ncpusx16gb. To request a full GPU node:

            #PBS -l select=1:ncpus=16:mem=250gb:ngpus=1:pcmem=16gb

        To request a single node with two GPUs:

            #PBS -l select=1:ncpus=16:mem=250gb:ngpus=2:pcmem=16gb
</code></pre>
<p>An example of a GPU pbs using pytorch is as follows</p>
<pre><code class="language-bash">#!/bin/bash

# --------------------------------------------------------------

### PART 1: Requests resources to run your job.

# --------------------------------------------------------------

#PBS -q standard

#PBS -l select=1:ncpus=28:mem=168gb:pcmem=6gb:np100s=1:os7=True

### Specify a name for the job

#PBS -N pytorch_gpus_single3

### Specify the group name

#PBS -W group_list=lecondon

### Walltime is how long your job will run

#PBS -l walltime=00:05:00

# --------------------------------------------------------------

### PART 2: Executes bash commands to run your job

# --------------------------------------------------------------

### Load required modules/libraries if needed

module load python/3.6/3.6.5

### In case needed for gpu..?

module load pytorch/nvidia/20.01

### Activate virtual environment

source ~/mypyenv_oc_py3.6/bin/activate

### change to your script’s directory

cd ~/python

### Run your work

singularity exec --nv /cm/shared/uaapps/pytorch/20.01/nvidia-pytorch.20.01-py3.simg python pytorch_gpus.py

sleep 10
</code></pre>
<ol>
<li>Submit job via qsub<ul>
<li><code>qsub pytorch_gpus_n1.pbs2708168.head1.cm.cluster</code></li>
</ul>
</li>
<li>Follow job progression via qstat<ul>
<li><code>qstat pbs2708168</code></li>
</ul>
</li>
<li>View results of job <code>&lt;jobname&gt;.o&lt;number&gt;</code> for results, <code>&lt;jobname&gt;.e&lt;number&gt;</code> for errors</li>
</ol>
<h2 id="how-to-run-python-in-a-jupiter-notebook-on-uahpc">How to run Python in a Jupiter notebook on UAHPC</h2>
<p>It might be easiest to run Python using a Jupyter Notebook, for which UAHPC has developed super convenient GUIs. Check <a href="https://public.confluence.arizona.edu/display/UAHPC/Jupyter+Notebook+-+Python">here</a> for a great tutorial. </p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../../../group_meetings/" class="btn btn-neutral float-left" title="Group Meetings"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../cheyenne/" class="btn btn-neutral float-right" title="Running on Cheyenne (NCAR)">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../../../group_meetings/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../cheyenne/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../../..";</script>
    <script src="../../../js/theme_extra.js"></script>
    <script src="../../../js/theme.js"></script>
      <script src="../../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
